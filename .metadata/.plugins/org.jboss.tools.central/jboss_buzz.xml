<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>How to program a multitenant SaaS platform in Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/22/how-program-multitenant-saas-platform-kubernetes" /><author><name>Bob Reselman</name></author><id>5982b94e-d5cc-4ec0-aea6-71f90efa4199</id><updated>2022-08-22T07:00:00Z</updated><published>2022-08-22T07:00:00Z</published><summary type="html">&lt;p&gt;In a &lt;a href="https://developers.redhat.com/articles/2022/08/12/implement-multitenant-saas-kubernetes"&gt;previous article&lt;/a&gt;, I described how to create a SaaS platform in a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster. This article takes a detailed look inside the demonstration project that accompanied that earlier article. The demonstration project is the code base shared by all tenants using the SaaS platform. This article describes the structure of the demonstration application. You'll also see how to get the code up and running for multiple &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; tenants in a Kubernetes cluster, and how to expose the service to clients.&lt;/p&gt; &lt;p&gt;A key aspect of SaaS architecture is a generic code base used by all tenants running in the Kubernetes cluster. The application logic used by each tenant is encapsulated in a Linux container image that's declared within the definition of the tenant's Kubernetes deployment. We'll see multiple examples of this generic approach and its benefits in this article.&lt;/p&gt; &lt;p&gt;The Linux container for each tenant is configured by setting a standard set of environment variables to values specific to that tenant. As shown in the previous article, adding a new tenant to the SaaS platform involves nothing more than setting up a database and creating a Kubernetes Secret, deployment, service, and route resources, all of which are assigned to a Kubernetes namespace created especially for the tenant. The result is that a single code base can support any number of tenants.&lt;/p&gt; &lt;h2&gt;Purpose of the demonstration project&lt;/h2&gt; &lt;p&gt;The demonstration project (which you can &lt;a href="https://github.com/redhat-developer-demos/instrument-resellers/"&gt;download from GitHub&lt;/a&gt;) is an evolution of code that started out as a single, standalone application used by a company named Clyde's Clarinets. The company determined that its code logic was generic enough to be converted to a SaaS platform that could support a number of other instrument resellers. Thus, Clyde's Clarinets became the Instrument Resellers SaaS platform.&lt;/p&gt; &lt;p&gt;The Resellers SaaS platform enables each vendor to acquire, refurbish, and resell a type of musical instrument special to that vendor. The Instrument Resellers demonstration project supports three vendors: Clyde's Clarinets, Betty's Brass, and Sidney's Saxophones (see Figure 1.)&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch_6.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/arch_6.png?itok=PrS6HW8G" width="385" height="565" alt="Web sites for multiple tenants using different namespaces can be supported by a single platform." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Web sites for multiple tenants using different namespaces can be supported by a single platform. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The sections that follow describe the design and code of the Instrument Reseller SaaS platform. Be advised that the demonstration code is still a work in progress. Most of the initial design work has been done and the endpoints for the &lt;code&gt;GET&lt;/code&gt; methods to the API resources are fully functional. Also, the project ships with a data seeding feature that makes it possible for each tenant's API instance to return instrument data that is specific to the reseller. However, the workflow code that moves an instrument through its phases, from Acquisition to Refurbishment and finally to Purchase, still needs to be written. There also has to be logic for &lt;code&gt;POST&lt;/code&gt;, &lt;code&gt;PUT&lt;/code&gt;, and &lt;code&gt;DELETE&lt;/code&gt; actions in the API.&lt;/p&gt; &lt;p&gt;Still, for the purposes of this article, the demonstration code provides a good starting place to understand how to use Kubernetes namespaces to implement a multitenant application in a single Kubernetes cluster. Once you get the code up and running in a Kubernetes cluster, you can experiment with adding the features that still need to be implemented.&lt;/p&gt; &lt;h2&gt;Designing the Instrument Resellers SaaS Platform&lt;/h2&gt; &lt;p&gt;The Instrument Resellers SaaS platform is written using the &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; framework. The code exposes a RESTful API that represents resources for acquiring, refurbishing, and reselling musical instruments. Each tenant using the SaaS platform exposes an instance of the API. For example, Sidney's Saxophones has its own instance of the API, as does Clyde's Clarinets and Betty's Brass. Figure 2 shows a user interface (UI) that exposes the &lt;code&gt;GET&lt;/code&gt; operations in Sidney's Saxophones.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ui_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ui_2.png?itok=x14sAN4I" width="1153" height="622" alt="Each instance of a vendor on the Instrument Resellers SaaS platform is represented by a dedicated RESTful API." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Each instance of a vendor on the Instrument Resellers SaaS platform is represented by a dedicated RESTful API. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The structure of the API is generic and is described in a &lt;a href="https://github.com/reselbob/instrument-resellers/blob/main/src/api/openapi.yaml"&gt;YAML file&lt;/a&gt; formatted according to the &lt;a href="https://swagger.io/specification/"&gt;OpenAPI 3.0 specification&lt;/a&gt;. As mentioned previously, the purpose of the Instrument Resellers SaaS is to allow a vendor to acquire, refurbish, and resell an instrument. The API represents these generic resources as Acquisitions, Refurbishments, and Purchases. (The Purchases resource describes instruments that have been sold via resale.)&lt;/p&gt; &lt;p&gt;The underlying logic assumes that once an instrument is acquired it will need to be refurbished, even if it only requires a cleaning. A Refurbishment is specified with &lt;code&gt;startDate&lt;/code&gt; and &lt;code&gt;finishDate&lt;/code&gt; properties. Once a Refurbishment is assigned a &lt;code&gt;finishDate&lt;/code&gt;, the associated instrument is ready for sale. Thus, the availability of an instrument for sale is determined through implication: if it has a &lt;code&gt;finishDate&lt;/code&gt; value, it can be sold. Once an instrument is sold, it becomes a Purchase resource.&lt;/p&gt; &lt;p&gt;There is a good argument to be made that instead of relying upon inference to determine that a Refurbishment is ready for sale, the developer experience would be improved through a more explicit approach, such as moving the instrument into a RESTful resource named Inventory. Making such a change would not degrade the generic status of the API, and creating an Inventory resource would not corrupt the intention of the SaaS, because all resellers—whether they're reselling clarinets, drums, or guitars—could support an Inventory resource. However, the internal logic in the source code for the SaaS would need to be altered to create an Inventory resource once a Refurbishment is assigned a &lt;code&gt;finishDate&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Again, making such changes would be OK because the change is generic. The important thing to remember is that keeping the API generic is essential to the design of the SaaS. Were the API to become too explicit, it would become brittle and lose its value for a SaaS platform.&lt;/p&gt; &lt;h2&gt;Defining a versatile data schema&lt;/h2&gt; &lt;p&gt;The need for a generic approach also holds true when designing the various data schemas used in the SaaS. Figure 3 shows the schemas used by the Instrument Resellers SaaS Platform.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/schema.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/schema.png?itok=gmVxkdaH" width="979" height="589" alt="The platform defines a schema for each resource (Instrument, etc.)." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The platform defines a schema for each resource (Instrument, etc.). &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The platform defines a schema for each resource.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The important thing to note about the data schemas is that the data types are generic. Along with scalar types such as &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;decimal&lt;/code&gt;, there are complex types for &lt;code&gt;Manufacturer&lt;/code&gt;, &lt;code&gt;Address&lt;/code&gt;, &lt;code&gt;User&lt;/code&gt;, &lt;code&gt;Acquisition&lt;/code&gt;, &lt;code&gt;Refurbishment&lt;/code&gt;, and &lt;code&gt;Purchase&lt;/code&gt;. The complex types are designed to apply to all instrument types and locales. Thus, the &lt;code&gt;Address&lt;/code&gt; type can be used for a location in the U.S. as well as a location in France. Also, the &lt;code&gt;Instrument&lt;/code&gt; type uses the data type &lt;code&gt;string&lt;/code&gt; to describe the &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;instrument&lt;/code&gt;, and &lt;code&gt;type&lt;/code&gt; properties.&lt;/p&gt; &lt;p&gt;To see how a particular tenant uses the schema, we'll retrieve the information for one clarinet offered by Clyde's Clarinets:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ curl clydesclarinets.local/v1/instruments/62bb6dde9b6a0fb486702123&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output from this command shows that the clarinet has a type of standard B flat and the name Excellent Clarinet. The instrument was manufactured by Hanson Clarinet Company. The address of Hanson Clarinet Company is displayed in JSON assigned to the address property.&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;{ "_id": "62bb6dde9b6a0fb486702123", "name": "Excellent Clarinet", "instrument": "clarinet", "type": "standard B flat", "manufacturer": { "name": "Hanson Clarinet Company", "description": "Noted for their clarinets which are made in their workshops in Marsden, West Yorkshire", "address": { "address_1": "Warehouse Hill", "address_2": "Apt. 745", "city": "Marsden", "state_province": "West Yorkshire", "zip_region_code": "HD7 6AB", "country": "UK", "_id": "62bb6dde9b6a0fb486702125", "created": "2022-06-28T21:08:46.549Z" }, "_id": "62bb6dde9b6a0fb486702124" }, "__v": 0 }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using a string for the &lt;code&gt;type&lt;/code&gt; property allows any vendor to describe an instrument with a good deal of distinction, regardless of whether the instrument is a guitar, a drum, or a saxophone.&lt;/p&gt; &lt;p&gt;Again, the trick with defining data structures is to keep the schemas generic. Just like the API, if a schema becomes too specific, it becomes brittle and subject to breakage.&lt;/p&gt; &lt;h2&gt;Deploying the demonstration application&lt;/h2&gt; &lt;p&gt;Now that you've learned about the structure of the RESTful API published by the Instrument Reseller SaaS platform as well as the data schemas used by the API, you're ready to get the demonstration code up and running.&lt;/p&gt; &lt;h3&gt;Identifying the runtime environment&lt;/h3&gt; &lt;p&gt;The demonstration code is intended to run on an instance of the &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; or &lt;a href="https://docs.fedoraproject.org/en-US/fedora/latest/install-guide/"&gt;Fedora&lt;/a&gt; operating system with &lt;a href="https://microshift.io/"&gt;MicroShift&lt;/a&gt; installed. MicroShift is a scaled-back version of the &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;. The reason we use MicroShift is that it offers a convenient way to deploy Kubernetes while providing the OpenShift &lt;a href="https://docs.openshift.com/container-platform/3.11/rest_api/route_openshift_io/route-route-openshift-io-v1.html"&gt;route&lt;/a&gt; resource, which provides an easy way to bind a Kubernetes service to a public URL that is accessible from outside the Kubernetes cluster. This &lt;a href="https://microshift.io/docs/getting-started/"&gt;Getting Started page&lt;/a&gt; explains how to install MicroShift on a Red Hat operating system.&lt;/p&gt; &lt;p&gt;Also, the demonstration code is designed to use a MongoDB database. A later section shows some of the details of working with MongoDB.&lt;/p&gt; &lt;p&gt;The deployment process is facilitated by using Kubernetes manifest files. You'll examine the details of the manifest files in a moment. But first, let's cover the demonstration project's data seeding feature.&lt;/p&gt; &lt;h3&gt;Data seeding&lt;/h3&gt; &lt;p&gt;The demonstration application seeds itself with random data that is particular to the instrument type that each reseller supports. For example, when the reseller Clyde's Clarinets is deployed into the Kubernetes cluster, the deployment seeds that reseller with data about acquiring, refurbishing, and reselling clarinets. The Sidney's Saxophones deployment seeds data relevant to saxophones. Betty's Brass is seeded with data relevant to brass instruments.&lt;/p&gt; &lt;p&gt;The purpose of data seeding is to provide a concrete way to understand multitenancy during this demo application. When you exercise the API for Clyde's Clarinets, you'll see only data relevant to Clyde's Clarinets. The same is true for Betty's Brass and Sidney's Saxophones. Seeing relevant data in a concrete manner makes it easier to understand the concept behind supporting multiple tenants in a SaaS platform.&lt;/p&gt; &lt;p&gt;The actual seeding process is facilitated by a Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/"&gt;init container&lt;/a&gt; that executes as part of the Kubernetes deployment. An init container is a container that runs before any other containers that are part of the Kubernetes pod. Our particular init container seeds the MongoDB database defined for the given reseller by loading the database with random data appropriate for the tenant (see Figure 4.)&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/seed.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/seed.png?itok=vW00WJr5" width="415" height="223" alt="The demonstration code for the Instrument Resellers SaaS platform seeds data particular to a given reseller." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The demonstration code for the Instrument Resellers SaaS platform seeds data particular to a given reseller. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The data seeding pattern has one small risk because the init container runs for every replica in the cluster. Unless some precaution is taken, the init container will write more data to the database each time a new replica starts, even though we need to run the init container only once. So the seeder used in the demonstration project checks for the existence of seed data and refrains from adding redundant entries.&lt;/p&gt; &lt;p&gt;The application binds to the MongoDB server via a connection string URL. That URL can represent a MongoDB server running internally within the Kubernetes cluster or external to the cluster using a service such as &lt;a href="https://www.mongodb.com/atlas/database"&gt;MongoDB Atlas&lt;/a&gt;. For simplicity's sake, the demonstration code was tested using a MongoDB instance running on MongoDB Atlas. Each tenant in the SaaS platform, in this example, is bound to a MongoDB instance as part of the process of configuring the Kubernetes manifest file for the given tenant's deployment.&lt;/p&gt; &lt;h3&gt;Getting Kubernetes manifest files&lt;/h3&gt; &lt;p&gt;The logic that powers each tenant in the Instrument Resellers SaaS platform is encapsulated in container images that are stored on the &lt;a href="https://quay.io"&gt;Quay&lt;/a&gt; container registry. You don't need to fiddle with source code directly to get an instrument reseller up and running. But you do need to configure the containers.&lt;/p&gt; &lt;p&gt;Configuration operates through Kubernetes manifest files that specify properties in YAML. The configuration files for this example are stored in a GitHub source code repository.&lt;/p&gt; &lt;p&gt;To get the manifest files, go to a terminal window on the machine in which MicroShift is running and execute the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ git clone https://github.com/redhat-developer-demos/instrument-resellers&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command copies the source code down from the GitHub repository that hosts the demonstration project.&lt;/p&gt; &lt;p&gt;Once the code is cloned from GitHub, navigate into the source code directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ cd instrument-resellers&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You're now ready to configure the Kubernetes manifest files.&lt;/p&gt; &lt;h3&gt;Preparing the manifest files for deployment&lt;/h3&gt; &lt;p&gt;The manifest files that you'll use to create instrument resellers in the Kubernetes cluster are in the &lt;code&gt;instrument-resellers/openshift&lt;/code&gt; directory. There, you'll find the manifest file that creates a tenant for each instrument reseller. The declarations for the namespace, Secret, deployment, service, and route resources for the given reseller are combined into a single YAML file for that reseller. The manifest file for Clyde's Clarinets is named &lt;code&gt;clarinet.yaml&lt;/code&gt;, the manifest file for Betty's Brass is &lt;code&gt;brass.yaml&lt;/code&gt;, and the file for Sidney's Saxophones is &lt;code&gt;saxophone.yaml&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The essential task performed by each resource follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;namespace&lt;/code&gt;: Declares the namespace that is unique for the given reseller.&lt;/li&gt; &lt;li&gt;&lt;code&gt;deployment&lt;/code&gt;: Configures the init container that seeds the reseller data and the regular container that has the application logic.&lt;/li&gt; &lt;li&gt;&lt;code&gt;service&lt;/code&gt;: Exposes the reseller on the internal Kubernetes network.&lt;/li&gt; &lt;li&gt;&lt;code&gt;route&lt;/code&gt;: Provides the URL to access the reseller from outside the Kubernetes cluster.&lt;/li&gt; &lt;li&gt;&lt;code&gt;secret&lt;/code&gt;: Specifies the URL (with embedded username and password) that defines the connection to the external MongoDB instance in which the reseller's data is stored.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The following excerpt from &lt;code&gt;clarinet.yaml&lt;/code&gt; (which is &lt;a href="https://github.com/redhat-developer-demos/instrument-resellers/blob/main/openshift/clarinet.yaml"&gt;in the GitHub repository for this demo application&lt;/a&gt;) shows the declaration for the Kubernetes Secret that has the URL that will connect the application code for Clyde's Clarinets to its associated MongoDB instance. Note that the &lt;code&gt;stringData.url&lt;/code&gt; property is assigned the value &lt;code&gt;&lt;mongo-url-here&gt;&lt;/code&gt;. This value is a placeholder for the URL that will be provided by the developer.&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: v1 kind: Secret metadata: name: mongo-url namespace: clydesclarinets type: Opaque stringData: url: &lt;mongo-url-here&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The demonstration project ships with a utility script named &lt;code&gt;set_mongo_url&lt;/code&gt;. The script is provided as a convenience. Executing the script inserts the connection string URL in the manifest files for all the instrument resellers: Clyde's Clarinets, Betty's Brass, and Sidney's Saxophones.&lt;/p&gt; &lt;p&gt;Or course, the script assumes that all the resellers use the same instance of MongoDB. In a production situation, each instrument reseller might be bound to a different database. Thus, the connection URLs will differ among resellers. But in this example, for demonstration purposes, using a single MongoDB instance is fine. Both the seeder code and the API code for a given tenant know how to create their particular database within the MongoDB instance. The database name for each reseller is defined by configuring an environment variable named &lt;code&gt;RESELLER_DB_NAME&lt;/code&gt; that is common to all resellers.&lt;/p&gt; &lt;p&gt;The syntax for using the &lt;code&gt;set_mongo_url&lt;/code&gt; utility script follows. Substitute your own connection string URL for &lt;code&gt;&lt;connection_string&gt;&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ sh set_mongo_url &lt;connection_string&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thus, if you want to make all the resellers in the demonstration project use the MongoDB instance defined bythe URL &lt;code&gt;mongodb+srv://reseller_user:F1Tc4lO5IVAXYZz@cluster0.ooooeo.mongodb.net&lt;/code&gt;, execute the utility script like so:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ sh set_mongo_url mongodb+srv://reseller_user:F1Tc4lO5IVAXYZz@cluster0.ooooeo.mongodb.net&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Applying the manifest files to the Kubernetes cluster&lt;/h2&gt; &lt;p&gt;Once the MongoDB URL has been specified for all the manifest files using the utility script in the previous section, the next step is to apply the manifest file for each reseller to the Kubernetes cluster. Create an instance of each instrument reseller in the MicroShift Kubernetes cluster by executing a &lt;code&gt;kubectl apply&lt;/code&gt; command for that instrument reseller.&lt;/p&gt; &lt;p&gt;Run the following command to create the Clyde's Clarinets reseller:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ kubectl apply -f clarinet.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the following command to create the Betty's Brass reseller:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ kubectl apply -f brass.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the following command to create the Sidney's Saxophones reseller:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ kubectl apply -f saxophone.yaml&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Getting the application routes&lt;/h2&gt; &lt;p&gt;After the three resellers have been deployed using &lt;code&gt;kubectl apply&lt;/code&gt;, you need to get the URL through which clients can get access to each of them. OpenShift makes this task simple through the &lt;code&gt;oc get routes&lt;/code&gt; command. Go to the Red Hat Enterprise Linux or Fedora instance that hosts the MicroShift Kubernetes cluster, and execute that command for each reseller to get its route. The &lt;code&gt;HOST/PORT&lt;/code&gt; column in the output lists the route's URL.&lt;/p&gt; &lt;p&gt;To get the route to Clyde's Clarinets, execute:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ oc get route -n clydesclarinets&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should get output similar to:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD instrumentreseller clydesclarinets.local instrumentreseller 8088 None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To get the route to Betty's Brass, execute:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ oc get route -n bettysbrass&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should get output similar to:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD instrumentreseller bettysbrass.local instrumentreseller 8088 None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To get the route to Sidney's Saxophones, execute:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ oc get route -n sidneyssaxophones&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should get output similar to:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD instrumentreseller sidneyssaxophones.local instrumentreseller 8088 None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that all the routes are retrieved according to the relevant Kubernetes namespace. To get the route for Clyde's Clarinets, you had to use &lt;code&gt;-n clydesclarinets&lt;/code&gt;. To get the route for Betty's Brass, you had to specify the &lt;code&gt;bettysbrass&lt;/code&gt; namespace. And to get the route for Sidney's Saxophones you had to specify the &lt;code&gt;sidneyssaxophones&lt;/code&gt; namespace.&lt;/p&gt; &lt;p&gt;This all makes sense when you remember that tenant isolation in the Kubernetes cluster is achieved through namespaces. Access to the route for each of the instrument resellers is determined according to its particular namespace.&lt;/p&gt; &lt;h2&gt;Binding the route's domain name to the machine host&lt;/h2&gt; &lt;p&gt;The last thing that needs to be done to access a particular instrument reseller API within the Kubernetes cluster is to bind the domain name of each instrument reseller to the IP address of the machine on which the Kubernetes cluster is running. The domain name returned by the &lt;code&gt;oc get route -n &lt;namespace&gt;&lt;/code&gt; command is automatically mapped to the associated service within the Kubernetes cluster. However, outside of the cluster, an instrument reseller's domain name is nothing more than an arbitrary string. By default, the host computer has no understanding of how to route the domain name to an IP address—the host computer's IP address, in this case.&lt;/p&gt; &lt;p&gt;Domain naming is not magical. Whenever a domain name is in play, that name is bound to an IP address of a web server or load balancer somewhere. The scope of the domain name can vary. If the host with that domain name is on the Web, the domain name is bound to a particular IP address by a domain registrar and propagated to all the public domain name servers across the globe.&lt;/p&gt; &lt;p&gt;If the scope of the domain name is limited to a local network, that domain name is bound to an IP address on the local network by making an entry in that local network's domain name server. If the scope of the domain name is limited to a single computer, that domain name is bound to that computer's IP address through an entry in the &lt;code&gt;/etc/hosts&lt;/code&gt; file of the computer using the domain name. Because MicroShift includes a Domain Name System (DNS) server, it can find the IP address in the host's &lt;code&gt;/etc/hosts&lt;/code&gt; file.&lt;/p&gt; &lt;p&gt;With regard to the demonstration project, the scope of the domain names in the URLs retrieved from the Kubernetes cluster using &lt;code&gt;oc get route&lt;/code&gt; is local to the computer running the Kubernetes cluster. Thus, at the least, that domain name needs to be bound to the local machine by making an entry in the &lt;code&gt;/etc/hosts&lt;/code&gt; file. The following lines in &lt;code&gt;/etc/hosts&lt;/code&gt; bind the domain names of our three instrument resellers to a local host with an IP address of 192.168.86.32:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;192.168.86.32 clydesclarinets.local 192.168.86.32 bettysbrass.local 192.168.86.32 sidneyssaxophones.local&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once &lt;code&gt;/etc/hosts&lt;/code&gt; is updated, you can access the given instrument reseller using the cURL command on the computer hosting the Kubernetes cluster. For example, using the domain names retrieved by using &lt;code&gt;oc get routes&lt;/code&gt; earlier, you can query the &lt;code&gt;/healthcheck&lt;/code&gt; endpoint on each instrument reseller's API to determine whether the given instrument reseller service is up and running.&lt;/p&gt; &lt;p&gt;For instance, the following command performs a health check on Sidney's Saxophones:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ curl sidneyssaxophones.local/v1/healthcheck&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The call to the API endpoint should produce the following results:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;{ "date": "2022-07-05T20:20:28.519Z", "message": "Things are A-OK at Sidney's Saxophones" }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command performs a health check on Clyde's Clarinets:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ curl clydesclarinets.local/v1/healthcheck&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output is particular to the clarinet reseller:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;{ "date": "2022-07-05T20:24:57.710Z", "message": "Things are A-OK at Clyde's Clarinets" }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;An interesting point to note is that a port number isn't required along with the instrument reseller's domain name to query a reseller's API. The reason for this is that the server running OpenShift has intelligence that examines the domain name associated with the HTTP request and routes the request to the relevant tenant according to the domain name. This technique is called &lt;a href="https://en.wikipedia.org/wiki/Virtual_hosting#Name-based"&gt;name-based virtual hosting&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;When name-based virtual hosting is in force, multiple domain names can be served from the same network port. The server reads the host property in the incoming request's HTTP header and then maps the domain name defined in the header to an internal IP address and port number within the Kubernetes cluster, according to the particular domain name.&lt;/p&gt; &lt;p&gt;There is another interesting point about the calls to the health check. When you look at the source code that is common to all instrument reseller tenants running in the Kubernetes cluster, you'll see that the differences in responses between Sidney's Saxophones and Clyde's Clarinets are due to differences in configuration. The code running both instrument resellers is identical.&lt;/p&gt; &lt;h2&gt;Exercising the tenant APIs&lt;/h2&gt; &lt;p&gt;As mentioned many times in this article, a significant benefit of a multitenant SaaS platform is that one code base can support a variety of tenants. The queries performing health checks in the previous section are a good example of this benefit. The benefit becomes even more apparent when making calls to the resource endpoints of the API for a given instrument reseller.&lt;/p&gt; &lt;p&gt;For example, the following command asks Sidney's Saxophones API for Purchases:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ curl sidneyssaxophones.local/v1/purchases&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;[ { "_id": "62bb6d9215e72a14688a16ba", "purchaseDate": "2022-05-02T12:53:46.088Z", "created": "2022-06-28T21:07:30.044Z", "buyer": { "firstName": "Meghan", "lastName": "Boyer", "email": "Meghan_Boyer92@hotmail.com", "phone": "758-676-6625 x849", "userType": "BUYER", "address": { "address_1": "69709 Renner Plains", "address_2": "Suite 351", "city": "Vallejo", "state_province": "AZ", "zip_region_code": "38547", "country": "USA", "_id": "62bb6d9215e72a14688a16bc", "created": "2022-06-28T21:07:30.044Z" }, "_id": "62bb6d9215e72a14688a16bb", "created": "2022-06-28T21:07:30.044Z" }, "instrument": { "instrument": "saxophone", "type": "bass", "name": "Twin Saxophone", "manufacturer": { "name": "Cannonball Musical Instruments", "description": "Manufacturer of a wide range of musical instruments", "address": { "address_1": "625 E Sego Lily Dr.", "address_2": "Apt. 276", "city": "Sandy", "state_province": "UT", "zip_region_code": "84070", "country": "USA", "_id": "62bb6d9215e72a14688a16bf", "created": "2022-06-28T21:07:30.044Z" }, "_id": "62bb6d9215e72a14688a16be" }, "_id": "62bb6d9215e72a14688a16bd" }, "price": 827, "__v": 0 }, . . . ]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Likewise, the following command queries the Clyde's Clarinets API:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ curl clydesclarinets-clydesclarinets.cluster.local/v1/purchases&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output shows the Purchases for that reseller:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;[ { "_id": "62bb6dd79b6a0fb4867020a8", "purchaseDate": "2022-05-02T19:59:04.324Z", "created": "2022-06-28T21:08:39.301Z", "buyer": { "firstName": "Otha", "lastName": "Bashirian", "email": "Otha.Bashirian57@gmail.com", "phone": "(863) 541-6638 x8875", "userType": "BUYER", "address": { "address_1": "47888 Oren Wall", "address_2": "Apt. 463", "city": "Dublin", "state_province": "FL", "zip_region_code": "49394", "country": "USA", "_id": "62bb6dd79b6a0fb4867020aa", "created": "2022-06-28T21:08:39.302Z" }, "_id": "62bb6dd79b6a0fb4867020a9", "created": "2022-06-28T21:08:39.302Z" }, "instrument": { "instrument": "clarinet", "type": "soprano", "name": "Ecstatic Clarinet", "manufacturer": { "name": "Amati-Denak", "description": "A manufacturer of wind and percussion instruments, parts, and accessories.", "address": { "address_1": "Dukelská 44", "address_2": "Apt. 117", "city": "Kraslice", "state_province": "Sokolov", "zip_region_code": "358 01", "country": "CZ", "_id": "62bb6dd79b6a0fb4867020ad", "created": "2022-06-28T21:08:39.302Z" }, "_id": "62bb6dd79b6a0fb4867020ac" }, "_id": "62bb6dd79b6a0fb4867020ab" }, "price": 777, "__v": 0 }, . . . ]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The data differs by the type of instrument sold by the reseller. But when you look under the covers at the application logic, you'll see that the code used by Sidney's Saxophones and Clyde's Clarinets is identical. The queries illustrate yet another example of the beauty of a well-designed multitenant SaaS platform. One single code base supports as many tenants as the physical infrastructure can host.&lt;/p&gt; &lt;h2&gt;Lessons regarding SaaS&lt;/h2&gt; &lt;p&gt;The demonstration project described in the article shows that multitenant SaaS platforms offer significant benefits. First and foremost is that when designed properly, a single code base can support any number of tenants running in a SaaS platform. Instead of having to dedicate a number of developers to many different software projects, a SaaS platform requires only a single development team supporting a single code base. The cost savings and reduction in technical debt are noteworthy.&lt;/p&gt; &lt;p&gt;Adding a new tenant to a SaaS platform running under Kubernetes requires nothing more than identifying a data source and configuring a set of Kubernetes resources for the new tenant. Deployment can be a matter of minutes instead of hours or even days. By saving time, you will save money.&lt;/p&gt; &lt;p&gt;Yet, for all the benefits that a SaaS platform provides, it also creates challenges.&lt;/p&gt; &lt;p&gt;The first challenge is getting configuration settings right. One misconfigured URL to a database or one bad value assignment to an environment variable can incur hours of debugging time. Configuration settings always need to be correct. Hence, automation is a recommended best practice.&lt;/p&gt; &lt;p&gt;The second challenge concerns infrastructure considerations. Optimal performance requires that the physical infrastructure on which the code runs can support the tenant's anticipated load. This means making sure that the physical infrastructure has the CPU, storage, and network capacity to support all tenants and that the Linux containers running the application logic are configured to take advantage of the physical resources available. Achieving this diversity can be complicated when each tenant is using the same code base.&lt;/p&gt; &lt;p&gt;A &lt;a href="https://www.redhat.com/en/topics/microservices/what-is-a-service-mesh"&gt;service mesh&lt;/a&gt; can make the tenant more operationally resilient by implementing circuit breaking and rerouting in the Kubernetes cluster.&lt;/p&gt; &lt;p&gt;In conclusion, the key takeaways for making a multitenant SaaS platform work under Kubernetes and OpenShift are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Design code and data structures that are generic enough to support a variety of tenants.&lt;/li&gt; &lt;li&gt;Make sure that the environment hosting the SaaS platform is robust and resilient enough to support the loads of all clients.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/22/how-program-multitenant-saas-platform-kubernetes" title="How to program a multitenant SaaS platform in Kubernetes"&gt;How to program a multitenant SaaS platform in Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-08-22T07:00:00Z</dc:date></entry><entry><title type="html">Using RHOSAK from WildFly</title><link rel="alternate" href="https://wildfly.org//news/2022/08/19/RHOSAK/" /><author><name>Kabir Khan</name></author><id>https://wildfly.org//news/2022/08/19/RHOSAK/</id><updated>2022-08-19T00:00:00Z</updated><content type="html">INTRODUCTION (full name: Red Hat OpenShift Streams for Apache Kafka) is a cloud service hosted by Red Hat which makes setting up, managing and scaling Apache Kafka instances very easy. Also, you get the peace of mind of knowing the instances are patched with the latest security fixes. is an open source, distributed streaming platform that enables (among other things) the development of real-time, event-driven applications. WildFly integrates with Apache Kafka via the MicroProfile Reactive Messaging subsystem, which implements the . In this blog we will see how to write a simple application which sends and receives messages to/from a Kafka instance. We will then point to how you would be able to run the application locally, using the configuration contained in the application. After that, we will set up a RHOSAK instance, create a topic and deploy our application into OpenShift. An interesting point here is that we will save the server url and the credentials needed to connect to it in an OpenShift secret. We then map the secret via the MicroProfile Config subsystem. The end result is that we override values hard coded in the application (i.e. the ones we used for the standalone case) from an external source. The source code for the example can found at . It contains a README for the RHOSAK steps covered here. Let’s get started! THE APPLICATION The core part of the application is pretty straightforward, it is an @ApplicationScoped CDI bean called MessagingBean. The full source code can be found . We will just outline the most important points below: @Inject @Channel("to-kafka") private Emitter&lt;String&gt; emitter; This injects a MicroProfile Reactive Messaging Emitter into the bean. The @Channel annotation comes from MicroProfile Reactive Messaging, and allows us to send messages to the MicroProfile Reactive Messaging stream in its name (in this case the name is ‘to-kafka’). We send messages in the following method: public Response send(String value) { System.out.println("Sending " + value); emitter.send(value); return Response.accepted().build(); } This method is called from a class called which handles POST requests to add data. Next we have a method using the @Incoming annotation, again from MicroProfile Reactive Messaging, which receives messages from the ‘from-kafka’ MicroProfile Reactive Messaging stream. @Incoming("from-kafka") public void receive(String value) { System.out.println("Received: " + value); synchronized (recentlyReceived) { if (recentlyReceived.size() &gt; 3) { recentlyReceived.removeFirst(); } recentlyReceived.add(value); } } It adds the messages to a list containing the three most recent entries. UserResource contains a method handling GET requests which returns the contents of this list. Then we have a properties file at file which does the mapping to Kafka. The contents of the file are as follows: # This will be overwritten by the entries set up in the initialize-server.cli script mp.messaging.connector.smallrye-kafka.bootstrap.servers=localhost:9092 # Configure the 'to-kafka' channel to write to. We write String entries to the Kafka topic 'testing' mp.messaging.outgoing.to-kafka.connector=smallrye-kafka mp.messaging.outgoing.to-kafka.topic=testing mp.messaging.outgoing.to-kafka.value.serializer=org.apache.kafka.common.serialization.StringSerializer # Configure the 'from-kafka' channel we receive messages from. We receive String entries from Kafka topic 'testing' mp.messaging.incoming.from-kafka.connector=smallrye-kafka mp.messaging.incoming.from-kafka.topic=testing mp.messaging.incoming.from-kafka.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer # Configure Kafka group.id to prevent warn message - if not set, some default value is generated automatically. mp.messaging.connector.smallrye-kafka.group.id="microprofile-reactive-messaging-kafka-group-id" The formats of the property keys can be found in the documentation which also goes into more depth about what each entry means. In short we’re pointing to a Kafka instance running on localhost:9092, which is the default port Kafka will run on. We’re pointing the @Channel(“to-kafka”) annotated Emitter we saw earlier to Kafka’s testing topic, and pointing the @Incoming(“from-kafka”) annotated receive() method to the same testing topic. Since both are using the same underlying Kafka topic, messages sent via the Emitter will be received in the receive() method. Finally, since we are sending Strings, we need to tell Kafka to use the String serializer/deserializer. RUNNING THE APPLICATION LOCALLY Since the intent of this article is to show integration with RHOSAK, we won’t go into too many details here, as it has been covered in this previous . The steps are: * Make sure WildFly is running, e.g. by one of the following two approaches * Download the latest WildFly zip. Note: it must be AT LEAST WildFly 27.0.0.Alpha4 since this project uses Jakarta EE dependencies, and prior to 27.0.0.Alpha4 WildFly was using the legacy Java EE dependencies. Enable the MicroProfile Reactive Messaging and Reactive Streams Operators extensions/subsystems by running the following operations in a CLI session: batch /extension=org.wildfly.extension.microprofile.reactive-messaging-smallrye:add /extension=org.wildfly.extension.microprofile.reactive-streams-operators-smallrye:add /subsystem=microprofile-reactive-streams-operators-smallrye:add /subsystem=microprofile-reactive-messaging-smallrye:add run-batch reload * Make sure you have a Kafka server running, for example by following steps 1 and 2 of the Kafka . * In a clone of run mvn package wildfly:deploy to build and deploy our application * Finally post messages to the application, and read them again by running the following commands in a terminal $ curl -X POST http://localhost:8080/wildfly-microprofile-reactive-messaging-rhosak-1.0.0-SNAPSHOT/one $ curl -X POST http://localhost:8080/wildfly-microprofile-reactive-messaging-rhosak-1.0.0-SNAPSHOT/two $ curl http://localhost:8080/wildfly-microprofile-reactive-messaging-rhosak-1.0.0-SNAPSHOT [one, two] You may now stop WildFly and Kafka. RUNNING WILDFLY IN OPENSHIFT WITH KAFKA PROVIDED BY RHOSAK SETTING UP A KAFKA INSTANCE ON RHOSAK AND CREATING A SECRET WITH CONNECTION INFORMATION First you need to set up a Kafka instance on RHOSAK. Since the rhoas line client is still under active development, the exact instructions how to do so might change. So rather than summarising everything you need to do here, see the section of the example application repository for how to install the rhoas client. Once you have the rhoas client installed, follow the following (again from the example application repository) to perform the following steps. * Login to RHOSAK * Create a Kafka instance, and set it as the active instance * Create a Kafka topic * Create a service account used to authenticate with the Kafka instance, and grant it access to produce/consume messages on the Kafka instance * Create an OpenShift secret called rhoas containing * the address of the Kafka instance * the service account details The secret will be called rhoas and contains the following entries: * KAFKA_HOST - the address and port of the Kafka instance running on RHOSAK * RHOAS_SERVICE_ACCOUNT_CLIENT_ID - the id of the service account used to authenticate with the Kafka instance * RHOAS_SERVICE_ACCOUNT_CLIENT_SECRET - the secret used to log in the client * RHOAS_SERVICE_ACCOUNT_OAUTH_TOKEN_URL - ignored in this example ADDITIONAL APPLICATION CONFIGURATION TO RUN IN OPENSHIFT AND CONNECT TO RHOSAK Although we are not quite ready to deploy our application yet, it is worth knowing that we will be using to deploy our application to OpenShift. To deploy an application using Helm, you use . The Helm chart for our application can be found at , and has the following contents: build: uri: https://github.com/kabir/vlog-mp-reactive-messaging-rhosak.git mode: bootable-jar deploy: replicas: 1 volumeMounts: - name: rhoas mountPath: /etc/config/rhoas readOnly: true volumes: - name: rhoas secret: secretName: rhoas This tells it to build a of WildFly, which is a single jar containing both the relevant parts of WildFly and our application. Further, it says to only create one pod running WildFly, and mounts the rhoas secret we created earlier under the directory /etc/config/rhoas on the pod running the server. This directory will contain a file for each entry in our secret. The file name will be the name of the entry, and the contents of the file will be the value of the entry. When deploying an application into OpenShift using Helm, it will look for a Maven profile called openshift in the application’s POM. The relevant part of our is: &lt;profile&gt; &lt;id&gt;openshift&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.wildfly.plugins&lt;/groupId&gt; &lt;artifactId&gt;wildfly-jar-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${version.wildfly-jar.maven.plugin}&lt;/version&gt; &lt;configuration&gt; &lt;feature-pack-location&gt;wildfly@maven(org.jboss.universe:community-universe)#${version.server.bootable-jar}&lt;/feature-pack-location&gt; &lt;layers&gt; &lt;layer&gt;cloud-server&lt;/layer&gt; &lt;layer&gt;microprofile-reactive-messaging-kafka&lt;/layer&gt; &lt;/layers&gt; &lt;plugin-options&gt; &lt;jboss-fork-embedded&gt;true&lt;/jboss-fork-embedded&gt; &lt;/plugin-options&gt; &lt;cli-sessions&gt; &lt;cli-session&gt; &lt;!-- do not resolve expression as they reference env vars that --&gt; &lt;!-- can be set at runtime --&gt; &lt;resolve-expressions&gt;false&lt;/resolve-expressions&gt; &lt;script-files&gt; &lt;script&gt;src/main/scripts/initialize-server.cli&lt;/script&gt; &lt;/script-files&gt; &lt;/cli-session&gt; &lt;/cli-sessions&gt; &lt;cloud/&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;package&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; The org.wildfly.plugins:wildfly-jar-maven-plugin plugin is used to create a bootable jar containing the application. We tell it to use the following when provisioning the server jar: * microprofile-reactive-messaging-kafka - this provides the MicroProfile Reactive Messaging functionality and the Kafka connector, as well as other dependencies needed by the Reactive Messaging implementation such as CDI. We briefly mentioned this layer in the section. * cloud-server - this is a trimmed down base server, whose main aim is to offer Jakarta RESTful Web Services functionality along with server dependencies needed to support those. The plugin will also run the WildFLy CLI script when configuring the server. It’s contents are: echo "Adding the 'rhoas' secret volume mount as a MicroProfile Config source..." /subsystem=microprofile-config-smallrye/config-source=rhosak-binding:add(dir={path=/etc/config/rhoas}) echo "Adding the MicroProfile Config entries mapping the secret values..." /subsystem=microprofile-config-smallrye/\ config-source=reactive-messaging-properties:add(properties={\ mp.messaging.connector.smallrye-kafka.bootstrap.servers=${KAFKA_HOST},\ mp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL,\ mp.messaging.connector.smallrye-kafka.sasl.mechanism=PLAIN,\ mp.messaging.connector.smallrye-kafka.sasl.jaas.config="\n\ org.apache.kafka.common.security.plain.PlainLoginModule required\n\ username=\"${RHOAS_SERVICE_ACCOUNT_CLIENT_ID}\"\n\ password=\"${RHOAS_SERVICE_ACCOUNT_CLIENT_SECRET}\";"\ }, ordinal=500) First of all it is worth noting that we don’t need to enable the MicroProfile Reactive Messaging and Reactive Streams Operators extensions/subsystems in this case. This is unlike when we were using the downloaded WildFly zip archive earlier. This is because when a server is provisioned using Galleon, the microprofile-reactive-messaging-kafka layer takes care of that for us. The first thing the CLI script does is mount the path /etc/config/rhoas (i.e. where our Helm chart told OpenShift to mount our rhoas secret) as a (in this case as a supported by our underlying SmallRye implementation of MicroProfile Config). After this config source is mounted, we can reference values from it in other places that can use MicroProfile Config values. This is what we are doing in the next block, where we tell WildFly’s MicroProfile Config subsystem to add the following properties: * mp.messaging.connector.smallrye-kafka.bootstrap.servers uses KAFKA_HOST from our rhoas secret. Adding this here overrides the value that we hardcoded in the earlier. * mp.messaging.connector.smallrye-kafka.security.protocol and mp.messaging.connector.smallrye-kafka.sasl.mechanism are used to secure the connection and enable authentication via SASL since RHOSAK is secured. The of the Kafka documentation explains these values in more detail. * mp.messaging.connector.smallrye-kafka.sasl.jaas.config sets up JAAS configuration to provide the RHOAS_SERVICE_ACCOUNT_CLIENT_ID and RHOAS_SERVICE_ACCOUNT_CLIENT_SECRET from our rhoas secret to autheniticate with RHOSAK. So in short the above configuration makes values from our secret available to WildFly, overrides the location of the Kafka server, and adds more MicroProfile Config properties to enable SSL and authentication. DEPLOYING OUR APPLICATION Now that we have configured everything properly, it is time to test our application! First you will need to install , and use it to add the wildfly Helm repostory as outlined in Then from the root folder of your local copy of the example repository, run: $ helm install rhosak-example -f ./helm.yml wildfly/wildfly This will return quickly but that does not mean the application is up and running yet. Check the application in the OpenShift console or using oc get deployment rhosak-example -w. Essentially what happens is it starts two pods. One for you application, and another which is doing the build of the bootable jar. Once the build one is done and has published the resulting image, the pod running the application can start properly. ACCESSING OUR APPLICATION RUNNING ON OPENSHIFT First we need the URL of our application on OpenShift: $ oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD rhosak-example rhosak-example-kkhan1-dev.apps.sandbox.x8i5.p1.openshiftapps.com rhosak-example &lt;all&gt; edge/Redirect None In my case the URL is rhosak-example-kkhan1-dev.apps.sandbox.x8i5.p1.openshiftapps.com. You should of course substitute that with the URL of your application in the following steps. Next, let’s add some entries using Curl: $ curl -X POST https://rhosak-example-kkhan1-dev.apps.sandbox.x8i5.p1.openshiftapps.com/one $ curl -X POST https://rhosak-example-kkhan1-dev.apps.sandbox.x8i5.p1.openshiftapps.com/two These will be sent to Kafka, and received again by the application which will keep a list of the most recently received values. Note that the https:// is needed - if left out, the commands will appear to work, but no data will actually be posted. To read this list of recently received values, we can run Curl again: $ curl https://rhosak-example-kkhan1-dev.apps.sandbox.x8i5.p1.openshiftapps.com [one, two] CONCLUSION Compared to running locally the RHOSAK steps look a lot more involved. However, we have achieved a lot! If we break down what we have actually done, it looks simpler: * Use rhoas to set up Kafka, a topic, and a service account authorised to publish/consume messages * Create a secret called rhoas containing the location of the Kafka instance and credentials to access it * Configure our application to use it by: * Mounting the secret under /etc/config/rhoas in the Helm Chart * Use org.wildfly.plugins:wildfly-jar-maven-plugin to * provision a trimmed down server with the required functionality * run a CLI script when building the server to mount the /etc/config/rhoas folder as a MicroProfile Config ConfigSource and use values from that to override the location of the server, and add properties to turn on SSL, SASL authentication, and provide the credentials from our secret to authenticate I hope this guide will be helpful to people wanting to try RHOSAK from WildFly for the first time.</content><dc:creator>Kabir Khan</dc:creator></entry><entry><title type="html">a DMN FEEL handbook</title><link rel="alternate" href="https://blog.kie.org/2022/08/a-dmn-feel-handbook.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2022/08/a-dmn-feel-handbook.html</id><updated>2022-08-18T07:10:49Z</updated><content type="html">a DMN FEEL handbook “promotion” video 🙂 We’re introducing an (experimental) DMN FEEL handbook, an helpful companion for your DMN modeling activities! You can access this new helpful resource at the following URL: . Key features include: * FEEL built-in functions organised by category * tested and integrated FEEL examples * Responsive design: easily access on Mobile, Tablet and Desktop from your favourite browser! …and many more! IMPLEMENTATION DETAILS For the technically curious, this section will highlight some of the technical implementation choices for the realisation of this handbook. If you want to just use this handbook, you are free to skip this section! 🙂 The framework used to render this handbook is called ; it’s very convenient to use it when, in general, you want a neat API documented with use-cases and examples. This makes it a perfect framework candidate to build a technical manual of a very specific aspect –the FEEL expression language of DMN. Naturally we’re keeping Antora for the general documentation of Drools, which is more powerful for the user-manuals and modularising content. Then, in order to CI the FEEL snippets and examples in the handbook, we’re leveraging the awesome ! As you can see in the Java_script source () we’re parsing the content of the Markdown file to search for codeblocks related to FEEL. Each codeblock related to FEEL is then evaluated, to make sure it does not error or misbehave. This allowed us to catch small typos, which we corrected in the main docs. I got the inspiration to implement this approach by reading the Rust manual. If you are familiar with that book, you will likely recognise where :). Finally, we integrated the JBang! tests with CI with a , and then we serve the content via GitHub Pages. CONCLUSIONS Don’t forget to checkout this DMN FEEL handbook today! Do you like it? Feedback? Let us know! The post appeared first on .</content><dc:creator>Matteo Mortari</dc:creator></entry><entry><title>A demonstration of Drogue IoT using Node.js</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/18/demonstration-drogue-iot-using-nodejs" /><author><name>Daniel Bevenius</name></author><id>a2f40cf1-6204-4671-b8d2-1c8c168bf5e1</id><updated>2022-08-18T07:00:00Z</updated><published>2022-08-18T07:00:00Z</published><summary type="html">&lt;p&gt;The goal of the &lt;a href="https://drogue.io/"&gt;Drogue IoT&lt;/a&gt; project is to make it easy to connect devices to cloud-based applications. This article will demonstrate how to implement firmware in Rust based on Drogue's device support. This way, a device can communicate with the cloud using the low power &lt;a href="https://lora-alliance.org/"&gt;LoRaWAN&lt;/a&gt; protocol. We will also illustrate how Node.js handles the server side.&lt;/p&gt; &lt;h2&gt;The purpose of Drogue IoT&lt;/h2&gt; &lt;p&gt;Many &lt;a href="https://developers.redhat.com/topics/open-source/"&gt;open source&lt;/a&gt; technologies already exist in the realm of messaging and the Internet of Things ( IoT). However, technologies change over time, and not everything that exists now is fit for the world of tomorrow. For instance, &lt;a href="https://developers.redhat.com/topics/c/"&gt;C and C++&lt;/a&gt; still have issues with memory safety. The concepts of cloud native, &lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;serverless&lt;/a&gt;, and pods might also need a different approach to designing cloud-side applications. Drogue IoT aims to help support these new environments.&lt;/p&gt; &lt;p&gt;&lt;a href="https://book.drogue.io/drogue-device/dev/index.html"&gt;Drogue Device&lt;/a&gt; is a firmware framework written in Rust with an actor-based programming model. &lt;a href="https://book.drogue.io/drogue-cloud/dev/index.html"&gt;Drogue Cloud&lt;/a&gt; is a thin layer of services that creates an IoT-friendly API for existing technologies such as &lt;a href="https://knative.dev/"&gt;Knative&lt;/a&gt; and &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; and a cloud-friendly API using &lt;a href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt; on the other side. The idea is to give you an overall solution ready to run IoT as a service. Figure 1 illustrates the Drogue IoT architecture.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch_5.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/arch_5.png?itok=D3a1IIwC" width="600" height="200" alt="An illustration of devices sending data that is transformed and exported." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Devices send data using standard protocols of Drogue Cloud, where they are transformed and exported. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;LoRaWAN network coverage&lt;/h2&gt; &lt;p&gt;LoRaWAN is a low-power wireless network that enables you to run a device on batteries for months, sending telemetry data to the cloud every now and then. To achieve this efficient connectivity, you need LoRaWAN network coverage, and &lt;a href="https://www.thethingsnetwork.org/"&gt;The Things Network&lt;/a&gt; (TTN) provides exactly that. You can extend the TTN network by running your gateway if your local area lacks coverage. TTN provides a public service that allows you to exchange data between devices and applications.&lt;/p&gt; &lt;h2&gt;Drogue Device&lt;/h2&gt; &lt;p&gt;Exchanging data with Drogue Device is easy. The following snippet focuses on the code  that exchanges data:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;let mut tx = String::&lt;heapless::consts::U32&gt;::new(); let led = match self.config.user_led.state().unwrap_or_default() { true =&gt; "on", false =&gt; "off", }; write!(&amp;mut tx, "ping:{},led:{}", self.counter, led).ok(); let tx = tx.into_bytes(); let mut rx = [0; 64]; let result = cfg .lora .request(LoraCommand::SendRecv(&amp;tx, &amp;mut rx)) .unwrap() .await;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice the &lt;code&gt;await&lt;/code&gt; keyword at the end? Yes, that is indeed asynchronous Rust. A hardware access layer (HAL) named &lt;a href="https://book.drogue.io/embassy/dev/index.html"&gt;Embassy&lt;/a&gt;, another Drogue IoT project, allows the program to run on the device, which in this example is an embedded &lt;a href="https://www.st.com/en/evaluation-tools/b-l072z-lrwan1.html"&gt;STM32 Cortex-M0 board&lt;/a&gt;. Thanks to Embassy and the drivers in Drogue Device, asynchronous programming becomes pretty simple. And thanks to Rust, your code is less likely to cause any undefined behavior, like corrupted memory.&lt;/p&gt; &lt;h2&gt;Node.js&lt;/h2&gt; &lt;p&gt;The cloud side of the IoT application needs a simple "reconcile loop." The device reports its current state, and you derive the desired state from that. The information received might result in a command you send back to the device.&lt;/p&gt; &lt;p&gt;The application in this article is pretty much the same as &lt;a href="https://developers.redhat.com/articles/2021/06/10/connect-quarkus-applications-drogue-iot-and-lorawan"&gt;connect-quarkus-applications-drogue-iot-and-lorawan&lt;/a&gt; written by Jens Reimann. But his version uses the Quarkus Java framework as the backend implementation, whereas our application uses Node.js.&lt;/p&gt; &lt;p&gt;The entry point of the application is &lt;code&gt;index.js&lt;/code&gt;, which configures and starts an HTTP server and an MQTT client. The HTTP server serves content from the static directory, which contains an &lt;code&gt;index.html&lt;/code&gt; file shown in the screenshot below. This file contains a &lt;code&gt;&lt;script&gt;&lt;/code&gt; element that uses Server Sent Events (SSE) to allow the server to send updates to it. In addition to serving the static content, the  HTTP server sends events through SSE. &lt;a href="https://www.fastify.io/"&gt;Fastify&lt;/a&gt; builds the server, and &lt;a href="https://www.npmjs.com/package/fastify-sse"&gt;fastify-sse&lt;/a&gt; handles the SSE.&lt;/p&gt; &lt;p&gt;The MQTT client handles a message event as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;client.on('message', (receiveTopic, message) =&gt; { const json = JSON.parse(message); const framePayload = Buffer.from(json.data.uplink_message.frm_payload, 'base64'); const event = { deviceId: json.device, timestamp: json.time, payload: framePayload.toString('utf8') }; sse.sendMessageEvent(event); if (event.payload.startsWith('ping')) { const command = { deviceId: event.deviceId, payload: getPayload(event, sse) }; sse.updateResponse(sse.lastResponse); sse.sendCommandEvent(command); const sendTopic = `command/${appName}/${command.deviceId}/port:1`; const responsePayload = Buffer.from(command.payload, 'utf8'); client.publish(sendTopic, responsePayload, {qos: QOS_AT_LEAST_ONCE}); } });&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Pretty simple, isn't it? For more details on the Node.js implementation, please see the &lt;a href="https://book.drogue.io/drogue-workshops/ttn-lorawan/nodejs-application.html"&gt;ttn-lorawan&lt;/a&gt; workshop.&lt;/p&gt; &lt;h2&gt;Drogue Cloud&lt;/h2&gt; &lt;p&gt;So far, the code shown in this article is fairly straightforward, focusing on our use case. However, we are missing a big chunk in the middle. How do we connect Node.js with the actual device? Sure, we could recreate all that ourselves, implementing the TTN API, registering devices, and processing events. Alternatively, we could simply use Drogue Cloud and let it do the plumbing for us.&lt;/p&gt; &lt;p&gt;Creating a new application and device is easy using the &lt;code&gt;drg&lt;/code&gt; command-line tool. Installation instructions are on the &lt;a href="https://github.com/drogue-iot/drg#installation"&gt;drg installation page&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ drg create application my-app $ drg create device --app my-app my-device&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The device registry in Drogue Cloud not only stores device information but can also reconcile with other services. Adding the following information makes it sync with TTN:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ drg create application my-app --spec '{ "ttn": { "api": { "apiKey": "...", "owner": "my-ttn-username", "region": "eu1" } } }' $ drg create --app my-app device my-device --spec '{ "ttn": { "app_eui": "0123456789ABCDEF", "dev_eui": "ABCDEF0123456789", "app_key": "0123456789ABCDEF...", "frequency_plan_id": "...", "lorawan_phy_version": "PHY_V1_0", "lorawan_version": "MAC_V1_0" } }'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This code creates a new TTN application, registers the device, sets up a webhook, creates the gateway configuration in Drogue Cloud, and ensures that credentials are present and synchronized.&lt;/p&gt; &lt;h2&gt;Learn more in the LoRaWAN end-to-end workshop&lt;/h2&gt; &lt;p&gt;Did that seem a bit fast? Yes, indeed! This is a lot of information for a single article, so we focused on the essential parts. We put together everything you need to know in the &lt;a href="https://book.drogue.io/drogue-workshops/ttn-lorawan/index.html"&gt;LoRaWAN end-to-end workshop&lt;/a&gt;, which provides more detail and background information. By the end of that workshop, you should have a web front-end to control your device, as shown in Figure 2. Most importantly, you will have a solid foundation for creating your own applications on top of Drogue IoT.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/web.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/web.png?itok=XIsruLc7" width="1440" height="523" alt="A screenshot of the application displaying messages received from the device." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The application displays messages received from the device. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;We hope you enjoyed this article. Now, you are ready to get started with Node.js and Drogue IoT. To learn more about what Red Hat is up to on the Node.js front, please explore our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/18/demonstration-drogue-iot-using-nodejs" title="A demonstration of Drogue IoT using Node.js "&gt;A demonstration of Drogue IoT using Node.js &lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniel Bevenius</dc:creator><dc:date>2022-08-18T07:00:00Z</dc:date></entry><entry><title>How to easily deploy JBoss EAP on Azure</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/17/how-ansible-simplifies-jboss-eap-deployment-azure" /><author><name>Harsha Cherukuri</name></author><id>508c1342-d157-4be8-9332-a27590fde406</id><updated>2022-08-17T07:00:00Z</updated><published>2022-08-17T07:00:00Z</published><summary type="html">&lt;p&gt;This article demonstrates how to deploy &lt;a href="https://developers.redhat.com/products/eap/download"&gt;Red Hat JBoss Enterprise Application Platform (JBoss EAP)&lt;/a&gt; on Microsoft Azure using &lt;a href="https://www.ansible.com"&gt;Ansible&lt;/a&gt; automation. Currently, Red Hat has a &lt;a href="https://azuremarketplace.microsoft.com/marketplace/apps/redhat.jboss-eap-rhel"&gt;Microsoft Azure Marketplace offering of JBoss EAP&lt;/a&gt;, but it's available only through the &lt;a href="https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/redhat/byos"&gt;Bring-Your-Own-Subscription (BYOS)&lt;/a&gt; model, which is relatively complex. This article creates Azure resources using &lt;a href="https://docs.ansible.com/ansible/latest/collections/azure/azcollection/index.html"&gt;Ansible Collections for Azure&lt;/a&gt; and then deploys JBoss EAP using the &lt;a href="https://github.com/ansible-middleware/wildfly"&gt;WildFly&lt;/a&gt; service provided by the &lt;a href="https://ansiblemiddleware.com/"&gt;Ansible Middleware project&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Deploy JBoss EAP in 6 easy steps&lt;/h2&gt; &lt;p&gt;We will use &lt;a href="https://github.com/ansible-middleware/azure-eap-demo"&gt;azure-eap-demo&lt;/a&gt; as the sample application for this article. Using this application, we will automate and deploy JBoss EAP instances on Azure virtual machines running &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Step 1.  Prerequisites setup&lt;/h3&gt; &lt;p&gt;To run the sample application, please put the following requirements in place:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An Azure account with an active subscription. If you do not have an Azure subscription, &lt;a href="https://azure.microsoft.com/pricing/free-trial"&gt;create one for free&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;JBoss EAP: You need a Red Hat account with a Red Hat Subscription Management entitlement for JBoss EAP. This entitlement allows you to download a version of JBoss EAP tested and certified by Red Hat. If you do not have a JBoss EAP entitlement, sign up for a free developer subscription:  &lt;a href="https://developers.redhat.com/register"&gt;Red Hat Developer Subscription for Individuals&lt;/a&gt;. Once registered, you can find the necessary credentials (pool IDs) at the &lt;a href="https://access.redhat.com/management/"&gt;Red Hat customer portal&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;The following software on the controller host or local machine: &lt;ul&gt; &lt;li&gt;Ansible (version 2.9 or greater)&lt;/li&gt; &lt;li&gt;Python (version 3.9 or greater)&lt;/li&gt; &lt;li&gt;Python3 &lt;a href="https://pypi.org/project/netaddr/"&gt;netaddr&lt;/a&gt; (obtained using dnf or pip)&lt;/li&gt; &lt;li&gt;The &lt;a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli"&gt;Azure command-line interface&lt;/a&gt; (CLI)&lt;/li&gt; &lt;li&gt;Download the &lt;a href="https://github.com/ansible-middleware/azure-eap-demo"&gt;azure-eap-demo application&lt;/a&gt; to your local machine.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Step 2.  Install WildFly and other components&lt;/h3&gt; &lt;p&gt;After you unpack the &lt;code&gt;azure-eap-demo&lt;/code&gt; application, change into the repository's top-level directory, and run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection install -r requirements.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We are using a dynamic inventory provided by Azure. Once the instances are created, you can view the inventory with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-inventory -i inventory/myazure_rm.yml --graph&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 3.  Create credentials for Red Hat portal access&lt;/h3&gt; &lt;p&gt;You need to provide credentials in the playbook so that it can download software from the Red Hat customer portal. Specify your Red Hat account name in the &lt;code&gt;rhn_username&lt;/code&gt; variable and your password in the &lt;code&gt;rhn_password&lt;/code&gt; variable.&lt;/p&gt; &lt;p&gt;In addition, you have to specify the Red Hat Subscription Management entitlement for JBoss EAP in the &lt;code&gt;jboss_eap_rhn_id&lt;/code&gt; variable. This variable allows you to specify which version of JBoss EAP (supported by Red Hat) you would like to install. Alternatively, you can just download and install the JBoss EAP ZIP file from the Red Hat customer portal.&lt;/p&gt; &lt;p&gt;All these variables can be stored in a YAML file, whose name you specify in a section in the Ansible playbook named &lt;code&gt;vars_files&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Step 4.  Run the Ansible playbook&lt;/h3&gt; &lt;p&gt;Now run the Ansible playbook in &lt;code&gt;create-demo-setup.yml &lt;/code&gt;which creates resources on Azure and deploys JBoss EAP:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;​​​​​​​$ ansible-playbook -e @rhn-creds.yml -i inventory/myazure_rm.yml -e "ansible_ssh_user=rheluser ansible_ssh_private_key_file='provide_your_ssh_private_key' hosts_group_name=eap wildfly_version=7.4 override_install_name=jboss-eap" create-demo-setup.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As part of the playbook execution, the &lt;a href="https://github.com/ansible-middleware/azure-eap-demo"&gt;azure-eap-demo&lt;/a&gt; repository is cloned. Its &lt;code&gt;create-demo-setup.yml&lt;/code&gt; file contains:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- name: Create Azure VM hosts: localhost gather_facts: false connection: local vars: repo_url: "https://github.com/ansible-middleware/wildfly-cluster-demo.git" branch: main tasks: - name: Git checkout ansible.builtin.git: repo: "{{ repo_url }}" dest: "{{ playbook_dir }}/wildfly-cluster-demo" version: "{{ branch }}" single_branch: yes clone: yes update: yes - name: Create demo resources on azure. include_role: name: 'azure' vars: ssh_key_path: "{{ ssh_key | default(lookup('env', 'HOME') + '/.ssh/id_rsa.pub')}}" - meta: refresh_inventory - pause: minutes: 1 - name: Run wildfly-cluster-demo import_playbook: wildfly-cluster-demo/playbook.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This playbook creates the Azure resources the application needs, including a resource group, virtual networks, subnets, security groups, network interfaces, three virtual machines running Red Hat Enterprise Linux, and public IP addresses for the virtual machines.&lt;/p&gt; &lt;p&gt;All the default parameters for the Azure cloud instances are within the installed package: &lt;code&gt;roles/azure/defaults/main.yml&lt;/code&gt; ​​​​​​.&lt;/p&gt; &lt;p&gt;Finally, the playbook deploys the WildFly cluster demo. Refer to the article &lt;a href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible"&gt;Automate and deploy a JBoss EAP cluster with Ansible&lt;/a&gt; to learn more about how to use WildFly.&lt;/p&gt; &lt;h3&gt;Step 5.  Verify deployment of the JBoss EAP cluster and application&lt;/h3&gt; &lt;p&gt;Once the playbook completes successfully, you can verify the JBoss EAP cluster by logging into the &lt;a href="https://portal.azure.com/"&gt;Azure portal&lt;/a&gt;. Here, you will find all the resources created to support the JBoss EAP cluster. Log in or SSH into any of the virtual machines created and confirm that the WildFly service is running and accessible. Alternatively, you can run the &lt;code&gt;validate.yml&lt;/code&gt; playbook provided in the &lt;a href="https://github.com/ansible-middleware/wildfly-cluster-demo"&gt;wildfly-cluster-demo&lt;/a&gt; application to validate the configuration.&lt;/p&gt; &lt;h3&gt;Step 6.  Clean up Azure resources&lt;/h3&gt; &lt;p&gt;To clean up all the resources created on Azure, run the &lt;code&gt;clean-demo-resources.yml&lt;/code&gt; playbook:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ansible-playbook clean-demo-resources.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The contents of &lt;code&gt;clean-demo-resources.yml&lt;/code&gt; are as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- name: Create Azure VM hosts: localhost connection: local tasks: - name: Create VM's on azure. include_role: name: 'azure' vars: action: destroy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;destroy&lt;/code&gt; action runs the &lt;code&gt;roles/azure/tasks/destroy.yml&lt;/code&gt; file, which contains:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: Remove a VM and all resources that were autocreated azure_rm_virtualmachine: resource_group: "{{ item.resourcegroup_name }}" name: "{{ item.name }}" remove_on_absent: all_autocreated state: absent loop: "{{ vm }}" - name: Delete a resource group including resources it contains azure_rm_resourcegroup: name: "{{ item.name }}" force_delete_nonempty: yes state: absent loop: "{{ resource_groups }}"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The playbook removes all the virtual machines and deletes all the resources under the &lt;code&gt;eap-cluster&lt;/code&gt; resource group.&lt;/p&gt; &lt;h2&gt;Ansible simplifies deployment on Azure&lt;/h2&gt; &lt;p&gt;In this article, we demonstrated a step-by-step process to create resources using Ansible on Microsoft Azure and deploy a JBoss EAP cluster using tooling from the Ansible Middleware project. Check out the other collections and demos within the &lt;a href="https://github.com/ansible-middleware"&gt;ansible-middleware&lt;/a&gt; GitHub organization and the &lt;a href="https://ansiblemiddleware.com"&gt;Ansible Middleware website&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/17/how-ansible-simplifies-jboss-eap-deployment-azure" title="How to easily deploy JBoss EAP on Azure"&gt;How to easily deploy JBoss EAP on Azure&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Harsha Cherukuri</dc:creator><dc:date>2022-08-17T07:00:00Z</dc:date></entry><entry><title>Quarkus Newsletter #23 - August</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-newsletter-23/&#xA;            " /><author><name>James Cobb (https://twitter.com/insectengine)</name></author><id>https://quarkus.io/blog/quarkus-newsletter-23/</id><updated>2022-08-17T00:00:00Z</updated><published>2022-08-17T00:00:00Z</published><summary type="html">The August Quarkus Newletter has been released! What’s new this month? Plenty! Get a great understanding of OpenTelemetry with Eric Deandrea’s article "OpenTelemetry: A Quarkus Superheroes demo of Observability". Get the good and bad experiences of a developer jumping into the deep end on a project with María Arias de...</summary><dc:creator>James Cobb (https://twitter.com/insectengine)</dc:creator><dc:date>2022-08-17T00:00:00Z</dc:date></entry><entry><title type="html">Getting started with AtlasMap</title><link rel="alternate" href="http://www.mastertheboss.com/java/getting-started-with-atlasmap/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/getting-started-with-atlasmap/</id><updated>2022-08-16T16:39:32Z</updated><content type="html">This article is a whirlwind tour of AtlasMap Data transformation API and User Interface. We will learn how to use its editor to define mapping rules and how to use them in a sample Java Integration project. Data transformation is the process of converting data from one format or structure into another format or structure. ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Connect MongoDB to a Node.js application with kube-service-bindings</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/16/connect-mongodb-nodejs-application-kube-service-bindings" /><author><name>Costas Papastathis, Michael Dawson</name></author><id>75fcba18-2deb-4b89-b6c2-c6e2fc0932a6</id><updated>2022-08-16T07:00:01Z</updated><published>2022-08-16T07:00:01Z</published><summary type="html">&lt;p&gt;This is the third and final article in a three-part series introducing &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt; for &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; developers on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. Together with the &lt;a href="https://operatorhub.io/operator/service-binding-operator"&gt;Service Binding Operator&lt;/a&gt; (SBO), kube-service-bindings makes it easier to share credentials for services with your applications.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2022/06/29/connect-services-kubernetes-easily-kube-service-bindings"&gt;first article of this series&lt;/a&gt; offered background on the tools we're using, and the &lt;a href="https://developers.redhat.com/articles/2022/07/22/enable-backing-services-kubernetes-service-service-binding-operator-and-kube"&gt;second&lt;/a&gt; set up some basic elements such as hosting on the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; and a &lt;a href="https://www.mongodb.com"&gt;MongoDB&lt;/a&gt; database. Now we're going to use all these tools to create a binding between our Node.js application and the database.&lt;/p&gt; &lt;h2&gt;Deploy MongoDB as a cloud-hosted database&lt;/h2&gt; &lt;p&gt;The previous article set up access between &lt;a href="https://www.mongodb.com/cloud/atlas/lp/try7?utm_source=google&amp;utm_campaign=gs_americas_united_states_search_core_brand_atlas_desktop&amp;utm_term=mongodb%20atlas&amp;utm_medium=cpc_paid_search&amp;utm_ad=e&amp;utm_ad_campaign_id=12212624338&amp;adgroup=115749704063&amp;gclid=Cj0KCQjw8uOWBhDXARIsAOxKJ2Hyk1VN_iVHDUUBWqUwwxLeVOw2WtbigZMayXJdUmnptIUnJUod8xAaAs1MEALw_wcB"&gt;MongoDB Atlas&lt;/a&gt; and your OpenShift cluster. If you went through those steps successfully, you are ready to deploy a cloud-hosted MongoDB database in the cluster as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In the OpenShift console, visit the Topology view by selecting &lt;strong&gt;Developer→Topology&lt;/strong&gt; from the left sidebar.&lt;/li&gt; &lt;li&gt;Select the project where you would like to deploy the cloud database by selecting, from the top of the left sidebar, &lt;strong&gt;Developer&lt;/strong&gt;→&lt;strong&gt;Topology&lt;/strong&gt;→&lt;strong&gt;Project&lt;/strong&gt;. From the dropdown menu, select your project.&lt;/li&gt; &lt;li&gt;In the left sidebar menu, click &lt;strong&gt;+Add→Cloud-Hosted Database→MongoDB Atlas Cloud Database Service→Add to Topology&lt;/strong&gt;. Select your database instance and click &lt;strong&gt;Add to topology→Continue&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Upon successful connection, you are taken to the Topology view, where the cloud-hosted database is deployed and visible (Figure 1).&lt;/li&gt; &lt;/ol&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/mongo_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/mongo_0.png?itok=oJytAkSx" width="341" height="284" alt="The Topology view shows that MongoDB Atlas is now accessible in your cluster." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The Topology view shows that MongoDB Atlas is now accessible in your cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Deploy the Node.js application in OpenShift&lt;/h2&gt; &lt;p&gt;There are several ways to deploy a Node.js application in OpenShift: Through the &lt;code&gt;oc&lt;/code&gt; OpenShift command-line interface (CLI), the &lt;code&gt;odo&lt;/code&gt; CLI, the OpenShift console, etc. This article covers two options: The OpenShift console and &lt;a href="https://www.npmjs.com/package/nodeshift"&gt;Nodeshift&lt;/a&gt;, an NPM package.&lt;/p&gt; &lt;h3&gt;Deploy through the OpenShift console&lt;/h3&gt; &lt;p&gt;From the &lt;strong&gt;Developer&lt;/strong&gt; perspective, select &lt;strong&gt;+Add→Import from Git&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In the &lt;strong&gt;Git Repo url&lt;/strong&gt; field, set the repository URL to &lt;code&gt;https://github.com/nodeshift-blog-examples/kube-service-bindings-examples&lt;/code&gt;. This is a &lt;a href="https://github.com/nodeshift-blog-examples/kube-service-bindings-examples"&gt;kube-service-bindings examples repository&lt;/a&gt; maintained by our team; it contains the Node.js application you are deploying in this article.&lt;/p&gt; &lt;p&gt;Expand &lt;strong&gt;Show advanced Git options&lt;/strong&gt;. On the &lt;strong&gt;Context dir&lt;/strong&gt; field, set the value to &lt;code&gt;src/mongodb&lt;/code&gt;, which is the path of the subdirectory where your Node.js application is located.&lt;/p&gt; &lt;p&gt;On &lt;strong&gt;Builder Image&lt;/strong&gt;, select &lt;strong&gt;Node.js&lt;/strong&gt; and click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt; &lt;h3&gt;Deploy through Nodeshift&lt;/h3&gt; &lt;p&gt;Open a terminal and clone the git repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/nodeshift-blog-examples/kube-service-bindings-examples.git $ cd ./kube-service-bindings-examples/src/mongodb&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Install Nodeshift globally:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ npm install -g nodeshift&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To find the login credentials required by the next command you'll use, visit your OpenShift console. In the upper right corner, click your username. A dropdown will appear. Click &lt;strong&gt;Copy login command&lt;/strong&gt; (Figure 2), which transfers you to another page. Then click &lt;strong&gt;Display Token&lt;/strong&gt; to display the username, password, and server credentials to log in with Nodeshift.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/copy_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/copy_1.png?itok=6SxN8H8y" width="378" height="209" alt="Under your name in the console, you can obtain login credentials." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Under your name in the console, you can obtain login credentials. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Using these credentials, you can now log in to your OpenShift cluster with Nodeshift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ nodeshift login --username=developer --password=password --server=https://api.server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Deploy the Node.js application with Nodeshift through the following command, replacing the namespace name with your specific project name:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ nodeshift --namespace.name=&lt;selected-project&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your application should be deployed and visible in the Topology view, as shown in Figure 3.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/node.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/node.png?itok=CJxGzdXs" width="227" height="236" alt="The Node.js application appears in the Topology view." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The Node.js application appears in the Topology view. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Establish a connection between the Node.js application and the MongoDB database&lt;/h2&gt; &lt;p&gt;The final step in this series is to establish a connection between your Node.js application and the MongoDB database, which we'll accomplish in this section.&lt;/p&gt; &lt;h3&gt;Service Binding Operator&lt;/h3&gt; &lt;p&gt;At this point, two instances should show up in your Topology view: the Node.js application and the connection to your MongoDB instance in Atlas (Figure 4).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/two_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/two_0.png?itok=uWDdr5gL" width="675" height="284" alt="The Topology view shows both the Node.js application and the external MongoDB database." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The Topology view shows both the Node.js application and the external MongoDB database. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;To establish a connection between these instances, you will use the Service Binding Operator to share the credentials and kube-service-bindings to parse those credentials (binding data).&lt;/p&gt; &lt;p&gt;You can create a Service Binding in two different ways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drag a line in the Topology view between the two backing services (the Node.js application and MongoDB).&lt;/li&gt; &lt;li&gt;Apply a YAML file specifying the service binding.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We will go with the first option, which in our case is faster and easier.&lt;/p&gt; &lt;p&gt;Hover the mouse over the Node.js application in the Topology view. An arrow should appear. Drag the arrow from the Node.js application to the circle around the MongoDB instance. A tooltip should be visible that says &lt;strong&gt;Create service binding&lt;/strong&gt;. Release the mouse button and a pop-up box will let you specify the name of the service binding. Click &lt;strong&gt;Create binding&lt;/strong&gt;. The container of the Node.js application will restart immediately (Figure 5).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Check the environment of Node.js application by clicking the Node.js application container in the Topology view. In the right sidebar, click &lt;strong&gt;Resources→View Logs (Pods Section)&lt;/strong&gt; and visit the &lt;strong&gt;Environment&lt;/strong&gt; tab. The &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; environment variable should be set, as shown in Figure 6.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;kube-service-bindings&lt;/h3&gt; &lt;p&gt;The final step is to read the binding data under the directory indicated by the &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; variable and pass the data to the MongoDB client to establish a connection to the MongoDB database. Your Node.js application already has kube-service-bindings as a dependency. So calling the &lt;code&gt;getBinding()&lt;/code&gt; function, as shown in the following JavaScript code snippet, does all the hard work of parsing, cleaning, and transforming the binding data into a consumable format for the MongoDB client:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;const { MongoClient } = require("mongodb"); const serviceBindings = require("kube-service-bindings"); const { url, connectionOptions } = serviceBindings.getBinding("MONGODB", "mongodb"); const mongoClient = new MongoClient(url, connectionOptions);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That's it. By visiting the URL of the Node.js application (click the arrow-box icon on the node), you can perform simple CRUD operations through the UI on the database.&lt;/p&gt; &lt;h2&gt;Easier integration with services on Kubernetes&lt;/h2&gt; &lt;p&gt;Over the past year, our team has been active in developing kube-service-bindings, making it easier for developers with little or no experience in managing containerized applications to securely share credentials among backing services.&lt;/p&gt; &lt;p&gt;Complementing the work on &lt;a href="https://github.com/nodeshift/kube-service-bindings"&gt;kube-service-bindings development&lt;/a&gt;, our team provides &lt;a href="https://github.com/nodeshift-blog-examples/kube-service-bindings-examples"&gt;examples&lt;/a&gt; for most of the clients supported by kube-service-bindings, instructions on utilizing kube-service-bindings, and a description of how to deploy a variety of backing services through Nodeshift in Kubernetes and OpenShift environments.&lt;/p&gt; &lt;p&gt;This series of articles has shown which clients are supported and how both a service binding and kube-service-bindings work. We guided you through the whole cycle of deploying a Node.js application backing service using the SBO and kube-service-bindings, sharing and parsing credentials for a connection between a Node.js application and a MongoDB database. kube-service-bindings read, parsed, and transformed binding data projected by the Service Binding Operator, returning data in a form directly consumable by the MongoDB client.&lt;/p&gt; &lt;p&gt;To help you use kube-service-bindings in other types of deployments, we have provided additional &lt;a href="https://github.com/nodeshift-blog-examples/kube-service-bindings-examples"&gt;Node.js examples&lt;/a&gt;. We hope you found this article interesting and now have a better understanding of kube-service-bindings and service bindings in general.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, check out &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;our Node.js page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/16/connect-mongodb-nodejs-application-kube-service-bindings" title="Connect MongoDB to a Node.js application with kube-service-bindings"&gt;Connect MongoDB to a Node.js application with kube-service-bindings&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Costas Papastathis, Michael Dawson</dc:creator><dc:date>2022-08-16T07:00:01Z</dc:date></entry><entry><title>How to set up Packit to simplify upstream project integration</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/16/how-set-packit-simplify-upstream-project-integration" /><author><name>Laura Barcziova</name></author><id>e727f048-6620-4039-9cca-83550b9309cc</id><updated>2022-08-16T07:00:00Z</updated><published>2022-08-16T07:00:00Z</published><summary type="html">&lt;p&gt;If you use open source projects from GitHub or GitLab in your infrastructure, you have probably established workflows to create project builds that you can install. You might create builds regularly and execute them in a similar manner across projects. Let's say you want to build the code changes in &lt;a href="https://getfedora.org"&gt;Fedora Linux&lt;/a&gt; or &lt;a href="https://www.centos.org/centos-stream/"&gt;CentOS Stream&lt;/a&gt; for each commit, release, or pull request in these projects. &lt;a href="https://packit.dev/"&gt;Packit&lt;/a&gt; can automate this policy, fold into a &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt; pipeline, and do even more.&lt;/p&gt; &lt;p&gt;Packit is an open source project that tests and builds RPM packages on Fedora Linux, CentOS Stream, and other distributions to ease the integration of upstream projects with the distributions.&lt;/p&gt; &lt;p&gt;This article focuses on &lt;a href="https://packit.dev/docs/packit-service/"&gt;Packit Service&lt;/a&gt;, which operates on GitHub and GitLab. You can also install a &lt;a href="https://packit.dev/docs/cli/"&gt;command line interface (CLI)&lt;/a&gt; locally to run Packit on your desktop or laptop.&lt;/p&gt; &lt;h2&gt;Setting up Packit in 3 steps&lt;/h2&gt; &lt;p&gt;Setting up Packit is pretty straightforward. Follow these three steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a valid &lt;a href="https://fedoraproject.org/wiki/Account_System"&gt;Fedora Account System &lt;/a&gt;account (if you don't already have one).&lt;/li&gt; &lt;li&gt;Install our GitHub application on &lt;a href="https://github.com/marketplace/packit-as-a-service"&gt;GitHub Marketplace&lt;/a&gt;, or &lt;a href="https://packit.dev/docs/guide/#how-to-set-up-packit-on-gitlab"&gt;configure a webhook&lt;/a&gt; on GitLab (depending on where your project lives).&lt;/li&gt; &lt;li&gt;Provide your FAS username, which will be verified (on Github, the verification is automatic).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That's all! Now you can add a configuration file to your project's repository and start setting up &lt;a href="https://packit.dev/docs/configuration/#packit-service-jobs"&gt;Packit service jobs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Most jobs require an &lt;a href="https://rpm-packaging-guide.github.io/#what-is-a-spec-file"&gt;RPM spec file&lt;/a&gt;. You can either place the spec file directly in your upstream repository or tell Packit how to download it from somewhere else.&lt;/p&gt; &lt;h2&gt;Packit for continuous integration&lt;/h2&gt; &lt;p&gt;Let's start with a simple example. You need to determine if the new code changes in GitHub or GitLab project built on all stable Fedora distros and CentOS Stream 9 will be successful. The process for such verifications is to get RPM builds for each pull request in your upstream project. The configuration file should include a build job like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;- job: copr_build trigger: pull_request targets: - fedora-stable-x86_64 - centos-stream-9-x86_64 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Easy, right? So what does this configuration accomplish? Packit takes the code changes for each pull request action, submits a new build in the &lt;a href="https://copr.fedorainfracloud.org/"&gt;Copr build system&lt;/a&gt;, and reports the results in the pull requests, as shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image1_10.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image1_10.png?itok=v9mQdDx1" width="925" height="347" alt="Packit creates commit checks that show results of the builds with links to details." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Packit creates commit checks that show results of the builds with links to details. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;You also have access to the actual Copr repository containing the builds. This access may be beneficial if you set up Packit to react to commits or releases because you can provide your RPM builds via the Copr repository to enable anyone to install them. You can also use Packit to do the builds in your Copr repository by specifying the owner and project name. Figure 2 shows one such repository.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image5_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image5_1.png?itok=BqZXrwOc" width="1161" height="564" alt="A Copr repository shows the latest builds for Cockpit projects." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. A Copr repository shows the latest builds for Cockpit projects. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Figure 3 shows a build by Packit in the repository.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image4_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image4_2.png?itok=VxBj-cem" width="809" height="498" alt="A Packit build displays general information about a Cockpit project build in the Copr repository." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. A Packit build displays general information about a Cockpit project build in the Copr repository. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Also, if you package your project in Fedora, you will be able to run scratch builds in a similar manner directly in &lt;a href="https://koji.fedoraproject.org"&gt;Koji&lt;/a&gt;, the Fedora build system.&lt;/p&gt; &lt;p&gt;Apart from the RPM builds, you can set up Packit to run your tests in the &lt;a href="https://docs.testing-farm.io/general/0.1/index.html"&gt;Testing Farm&lt;/a&gt; infrastructure. The tests can either use the built RPMs from Copr or run independently. You can read more information about tests in &lt;a href="https://packit.dev/docs/testing-farm/"&gt;the Packit documentation&lt;/a&gt;. Figure 4 shows how Packit reports test results.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image3_4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image3_4.png?itok=K4ciQ_LS" width="914" height="347" alt="Packit creates commit checks that show results of the tests with links to details." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Packit creates commit checks that show results of the tests with links to details. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Packit automation for Fedora downstream releases&lt;/h2&gt; &lt;p&gt;If you have ever maintained a package in Fedora, you probably know that releasing a new version downstream can be tedious. Packit can help you with boring tasks and do the repetitive work for you. With Packit, you can easily get your upstream releases into &lt;a href="https://src.fedoraproject.org"&gt;Fedora Package Sources&lt;/a&gt;, automatically submit builds in the &lt;a href="https://koji.fedoraproject.org/koji/"&gt;Koji build system&lt;/a&gt;, and create &lt;a href="https://bodhi.fedoraproject.org"&gt;Bodhi updates&lt;/a&gt;. If you are interested in this kind of automation, make sure to check our &lt;a href="https://packit.dev/docs/fedora-releases-guide/"&gt;release guide&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Ready to give Packit a try?&lt;/h2&gt; &lt;p&gt;If you are dealing with any of the situations mentioned, please check &lt;a href="https://packit.dev/docs/"&gt;our documentation&lt;/a&gt;, which will guide you through the Packit setup. If you have any questions, feel free to &lt;a href="https://packit.dev/#contact"&gt;contact us&lt;/a&gt;. We are happy to help and receive feedback or suggestions for features you would like to add.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/16/how-set-packit-simplify-upstream-project-integration" title="How to set up Packit to simplify upstream project integration"&gt;How to set up Packit to simplify upstream project integration&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Laura Barcziova</dc:creator><dc:date>2022-08-16T07:00:00Z</dc:date></entry><entry><title>How OpenShift Serverless Logic evolved to improve workflows</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/15/how-openshift-serverless-logic-evolved-improve-workflows" /><author><name>Daniel Oh, Simon Seagrave</name></author><id>8a532101-1b3e-4c4f-a8f4-e08ff8c689fe</id><updated>2022-08-15T07:00:00Z</updated><published>2022-08-15T07:00:00Z</published><summary type="html">&lt;p&gt;Serverless is an advanced cloud deployment model that aims to run business services on demand, enabling enterprises to save infrastructure costs tremendously. The benefit of serverless is an application designed and developed as abstract functions regardless of programming languages. This article describes how the serverless and function models have evolved since they were unleashed upon the world with AWS Lambda and what to look forward to with Red Hat OpenShift serverless logic.&lt;/p&gt; &lt;h2&gt;The 3 phases of serverless evolution&lt;/h2&gt; &lt;p&gt;As serverless technologies evolve, we at Red Hat created the evolutionary scale to help our customers better understand how serverless has grown and matured over time. The three phases of serverless evolution are as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Serverless 1.0 &lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p class="Indent1"&gt;At the beginning of the serverless era, the 1.0 phase, serverless was thought of as functions with tiny snippets of code running on demand for a short period. AWS Lambda made this paradigm popular, but it had limitations in terms of execution time, protocols, and runtimes.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Serverless 1.5&lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p class="Indent1"&gt;With the increase in popularity of &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;&lt;u&gt;Kubernetes&lt;/u&gt;&lt;/a&gt; running microservices on container platforms, the serverless era also moved forward to the 1.5 phase. This phase augmented serverless traits and benefits by deploying polyglot runtimes and container-based functions. The serverless 1.5 phase also delivered an abstraction layer to manage serverless applications using the open Kubernetes serverless community project, &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;&lt;u&gt;Knative&lt;/u&gt;&lt;/a&gt;. Red Hat was one of the founding members of the Knative project and continues to be one of the top contributors to that community. But this was not the end of the journey.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Serverless 2.0&lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p class="Indent1"&gt;We are now approaching the new serverless 2.0 phase. This phase involves more complex orchestration and integration patterns combined with some level of state management. Serverless functions are often thought of as stateless applications. But serverless workflows are designed for complex orchestrations of multiple services and functions, typically while preserving the state. The adoption of serverless increases as organizations perform more complex orchestrations. Consequently, the OpenShift Serverless team implements serverless workflows, utilizing our command of the business process automation space.&lt;/p&gt; &lt;h2&gt;The future of serverless workflows&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://serverlessworkflow.io/"&gt;&lt;u&gt;Serverless Workflow&lt;/u&gt;&lt;/a&gt; project is an open source specification that enables developers to design workflows running serverless functions using a standard domain-specific language (DSL). For increased flexibility, Serverless Workflows also allow developers to define business logic triggered by multiple events and services such as &lt;a href="https://cloudevents.io/"&gt;&lt;u&gt;CloudEvents&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://swagger.io/specification/"&gt;&lt;u&gt;OpenAPI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.asyncapi.com/"&gt;&lt;u&gt;AsyncAPI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://graphql.org/"&gt;&lt;u&gt;GraphQL&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://grpc.io/"&gt;&lt;u&gt;gRPC&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Red Hat is one of the &lt;a href="https://serverlessworkflow.io/community.html"&gt;&lt;u&gt;project maintainers&lt;/u&gt;&lt;/a&gt; of the Serverless Workflow project. We have been actively involved in innovation and contribution since the earliest days of the &lt;a href="https://www.cncf.io/"&gt;Cloud Native Computing Foundation (CNCF)&lt;/a&gt; project.&lt;/p&gt; &lt;p&gt;We are about to release a new feature of OpenShift Serverless called the serverless logic in developer preview. This feature allows developers to design workflows with serverless deployment and function development capabilities based on Knative and &lt;a href="https://kogito.kie.org/"&gt;&lt;u&gt;Kogito.&lt;/u&gt;&lt;/a&gt; Kogito augments function orchestration and automation to implement serverless workflows at scale on Red Hat OpenShift.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Stay tuned to the official &lt;a href="https://twitter.com/rhdevelopers"&gt;&lt;u&gt;@rhdevelopers Twitter stream&lt;/u&gt;&lt;/a&gt; and this blog for more details about our exciting new capability as we approach the release of Red Hat OpenShift Serverless Logic. We will also provide tutorials.&lt;/p&gt; &lt;p&gt;In the meantime, learn more about &lt;a href="https://developers.redhat.com/topics/serverless-java"&gt;OpenShift Serverless&lt;/a&gt;, and try it out by setting up your free and easy-to-use &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Red Hat Sandbox&lt;/a&gt; environment.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/15/how-openshift-serverless-logic-evolved-improve-workflows" title="How OpenShift Serverless Logic evolved to improve workflows"&gt;How OpenShift Serverless Logic evolved to improve workflows&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniel Oh, Simon Seagrave</dc:creator><dc:date>2022-08-15T07:00:00Z</dc:date></entry></feed>
