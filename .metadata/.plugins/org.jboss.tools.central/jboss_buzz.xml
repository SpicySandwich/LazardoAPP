<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Using RHOSAK from WildFly</title><link rel="alternate" href="https://wildfly.org//news/2022/08/19/RHOSAK/" /><author><name>Kabir Khan</name></author><id>https://wildfly.org//news/2022/08/19/RHOSAK/</id><updated>2022-08-19T00:00:00Z</updated><content type="html">INTRODUCTION (full name: Red Hat OpenShift Streams for Apache Kafka) is a cloud service hosted by Red Hat which makes setting up, managing and scaling Apache Kafka instances very easy. Also, you get the peace of mind of knowing the instances are patched with the latest security fixes. is an open source, distributed streaming platform that enables (among other things) the development of real-time, event-driven applications. WildFly integrates with Apache Kafka via the MicroProfile Reactive Messaging subsystem, which implements the . In this blog we will see how to write a simple application which sends and receives messages to/from a Kafka instance. We will then point to how you would be able to run the application locally, using the configuration contained in the application. After that, we will set up a RHOSAK instance, create a topic and deploy our application into OpenShift. An interesting point here is that we will save the server url and the credentials needed to connect to it in an OpenShift secret. We then map the secret via the MicroProfile Config subsystem. The end result is that we override values hard coded in the application (i.e. the ones we used for the standalone case) from an external source. The source code for the example can found at . It contains a README for the RHOSAK steps covered here. Let’s get started! THE APPLICATION The core part of the application is pretty straightforward, it is an @ApplicationScoped CDI bean called MessagingBean. The full source code can be found . We will just outline the most important points below: @Inject @Channel("to-kafka") private Emitter&lt;String&gt; emitter; This injects a MicroProfile Reactive Messaging Emitter into the bean. The @Channel annotation comes from MicroProfile Reactive Messaging, and allows us to send messages to the MicroProfile Reactive Messaging stream in its name (in this case the name is ‘to-kafka’). We send messages in the following method: public Response send(String value) { System.out.println("Sending " + value); emitter.send(value); return Response.accepted().build(); } This method is called from a class called which handles POST requests to add data. Next we have a method using the @Incoming annotation, again from MicroProfile Reactive Messaging, which receives messages from the ‘from-kafka’ MicroProfile Reactive Messaging stream. @Incoming("from-kafka") public void receive(String value) { System.out.println("Received: " + value); synchronized (recentlyReceived) { if (recentlyReceived.size() &gt; 3) { recentlyReceived.removeFirst(); } recentlyReceived.add(value); } } It adds the messages to a list containing the three most recent entries. UserResource contains a method handling GET requests which returns the contents of this list. Then we have a properties file at file which does the mapping to Kafka. The contents of the file are as follows: # This will be overwritten by the entries set up in the initialize-server.cli script mp.messaging.connector.smallrye-kafka.bootstrap.servers=localhost:9092 # Configure the 'to-kafka' channel to write to. We write String entries to the Kafka topic 'testing' mp.messaging.outgoing.to-kafka.connector=smallrye-kafka mp.messaging.outgoing.to-kafka.topic=testing mp.messaging.outgoing.to-kafka.value.serializer=org.apache.kafka.common.serialization.StringSerializer # Configure the 'from-kafka' channel we receive messages from. We receive String entries from Kafka topic 'testing' mp.messaging.incoming.from-kafka.connector=smallrye-kafka mp.messaging.incoming.from-kafka.topic=testing mp.messaging.incoming.from-kafka.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer # Configure Kafka group.id to prevent warn message - if not set, some default value is generated automatically. mp.messaging.connector.smallrye-kafka.group.id="microprofile-reactive-messaging-kafka-group-id" The formats of the property keys can be found in the documentation which also goes into more depth about what each entry means. In short we’re pointing to a Kafka instance running on localhost:9092, which is the default port Kafka will run on. We’re pointing the @Channel(“to-kafka”) annotated Emitter we saw earlier to Kafka’s testing topic, and pointing the @Incoming(“from-kafka”) annotated receive() method to the same testing topic. Since both are using the same underlying Kafka topic, messages sent via the Emitter will be received in the receive() method. Finally, since we are sending Strings, we need to tell Kafka to use the String serializer/deserializer. RUNNING THE APPLICATION LOCALLY Since the intent of this article is to show integration with RHOSAK, we won’t go into too many details here, as it has been covered in this previous . The steps are: * Make sure WildFly is running, e.g. by one of the following two approaches * Download the latest WildFly zip. Note: it must be AT LEAST WildFly 27.0.0.Alpha4 since this project uses Jakarta EE dependencies, and prior to 27.0.0.Alpha4 WildFly was using the legacy Java EE dependencies. Enable the MicroProfile Reactive Messaging and Reactive Streams Operators extensions/subsystems by running the following operations in a CLI session: batch /extension=org.wildfly.extension.microprofile.reactive-messaging-smallrye:add /extension=org.wildfly.extension.microprofile.reactive-streams-operators-smallrye:add /subsystem=microprofile-reactive-streams-operators-smallrye:add /subsystem=microprofile-reactive-messaging-smallrye:add run-batch reload * Make sure you have a Kafka server running, for example by following steps 1 and 2 of the Kafka . * In a clone of run mvn package wildfly:deploy to build and deploy our application * Finally post messages to the application, and read them again by running the following commands in a terminal $ curl -X POST http://localhost:8080/wildfly-microprofile-reactive-messaging-rhosak-1.0.0-SNAPSHOT/one $ curl -X POST http://localhost:8080/wildfly-microprofile-reactive-messaging-rhosak-1.0.0-SNAPSHOT/two $ curl http://localhost:8080/wildfly-microprofile-reactive-messaging-rhosak-1.0.0-SNAPSHOT [one, two] You may now stop WildFly and Kafka. RUNNING WILDFLY IN OPENSHIFT WITH KAFKA PROVIDED BY RHOSAK SETTING UP A KAFKA INSTANCE ON RHOSAK AND CREATING A SECRET WITH CONNECTION INFORMATION First you need to set up a Kafka instance on RHOSAK. Since the rhoas line client is still under active development, the exact instructions how to do so might change. So rather than summarising everything you need to do here, see the section of the example application repository for how to install the rhoas client. Once you have the rhoas client installed, follow the following (again from the example application repository) to perform the following steps. * Login to RHOSAK * Create a Kafka instance, and set it as the active instance * Create a Kafka topic * Create a service account used to authenticate with the Kafka instance, and grant it access to produce/consume messages on the Kafka instance * Create an OpenShift secret called rhoas containing * the address of the Kafka instance * the service account details The secret will be called rhoas and contains the following entries: * KAFKA_HOST - the address and port of the Kafka instance running on RHOSAK * RHOAS_SERVICE_ACCOUNT_CLIENT_ID - the id of the service account used to authenticate with the Kafka instance * RHOAS_SERVICE_ACCOUNT_CLIENT_SECRET - the secret used to log in the client * RHOAS_SERVICE_ACCOUNT_OAUTH_TOKEN_URL - ignored in this example ADDITIONAL APPLICATION CONFIGURATION TO RUN IN OPENSHIFT AND CONNECT TO RHOSAK Although we are not quite ready to deploy our application yet, it is worth knowing that we will be using to deploy our application to OpenShift. To deploy an application using Helm, you use . The Helm chart for our application can be found at , and has the following contents: build: uri: https://github.com/kabir/vlog-mp-reactive-messaging-rhosak.git mode: bootable-jar deploy: replicas: 1 volumeMounts: - name: rhoas mountPath: /etc/config/rhoas readOnly: true volumes: - name: rhoas secret: secretName: rhoas This tells it to build a of WildFly, which is a single jar containing both the relevant parts of WildFly and our application. Further, it says to only create one pod running WildFly, and mounts the rhoas secret we created earlier under the directory /etc/config/rhoas on the pod running the server. This directory will contain a file for each entry in our secret. The file name will be the name of the entry, and the contents of the file will be the value of the entry. When deploying an application into OpenShift using Helm, it will look for a Maven profile called openshift in the application’s POM. The relevant part of our is: &lt;profile&gt; &lt;id&gt;openshift&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.wildfly.plugins&lt;/groupId&gt; &lt;artifactId&gt;wildfly-jar-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${version.wildfly-jar.maven.plugin}&lt;/version&gt; &lt;configuration&gt; &lt;feature-pack-location&gt;wildfly@maven(org.jboss.universe:community-universe)#${version.server.bootable-jar}&lt;/feature-pack-location&gt; &lt;layers&gt; &lt;layer&gt;cloud-server&lt;/layer&gt; &lt;layer&gt;microprofile-reactive-messaging-kafka&lt;/layer&gt; &lt;/layers&gt; &lt;plugin-options&gt; &lt;jboss-fork-embedded&gt;true&lt;/jboss-fork-embedded&gt; &lt;/plugin-options&gt; &lt;cli-sessions&gt; &lt;cli-session&gt; &lt;!-- do not resolve expression as they reference env vars that --&gt; &lt;!-- can be set at runtime --&gt; &lt;resolve-expressions&gt;false&lt;/resolve-expressions&gt; &lt;script-files&gt; &lt;script&gt;src/main/scripts/initialize-server.cli&lt;/script&gt; &lt;/script-files&gt; &lt;/cli-session&gt; &lt;/cli-sessions&gt; &lt;cloud/&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;package&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; The org.wildfly.plugins:wildfly-jar-maven-plugin plugin is used to create a bootable jar containing the application. We tell it to use the following when provisioning the server jar: * microprofile-reactive-messaging-kafka - this provides the MicroProfile Reactive Messaging functionality and the Kafka connector, as well as other dependencies needed by the Reactive Messaging implementation such as CDI. We briefly mentioned this layer in the section. * cloud-server - this is a trimmed down base server, whose main aim is to offer Jakarta RESTful Web Services functionality along with server dependencies needed to support those. The plugin will also run the WildFLy CLI script when configuring the server. It’s contents are: echo "Adding the 'rhoas' secret volume mount as a MicroProfile Config source..." /subsystem=microprofile-config-smallrye/config-source=rhosak-binding:add(dir={path=/etc/config/rhoas}) echo "Adding the MicroProfile Config entries mapping the secret values..." /subsystem=microprofile-config-smallrye/\ config-source=reactive-messaging-properties:add(properties={\ mp.messaging.connector.smallrye-kafka.bootstrap.servers=${KAFKA_HOST},\ mp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL,\ mp.messaging.connector.smallrye-kafka.sasl.mechanism=PLAIN,\ mp.messaging.connector.smallrye-kafka.sasl.jaas.config="\n\ org.apache.kafka.common.security.plain.PlainLoginModule required\n\ username=\"${RHOAS_SERVICE_ACCOUNT_CLIENT_ID}\"\n\ password=\"${RHOAS_SERVICE_ACCOUNT_CLIENT_SECRET}\";"\ }, ordinal=500) First of all it is worth noting that we don’t need to enable the MicroProfile Reactive Messaging and Reactive Streams Operators extensions/subsystems in this case. This is unlike when we were using the downloaded WildFly zip archive earlier. This is because when a server is provisioned using Galleon, the microprofile-reactive-messaging-kafka layer takes care of that for us. The first thing the CLI script does is mount the path /etc/config/rhoas (i.e. where our Helm chart told OpenShift to mount our rhoas secret) as a (in this case as a supported by our underlying SmallRye implementation of MicroProfile Config). After this config source is mounted, we can reference values from it in other places that can use MicroProfile Config values. This is what we are doing in the next block, where we tell WildFly’s MicroProfile Config subsystem to add the following properties: * mp.messaging.connector.smallrye-kafka.bootstrap.servers uses KAFKA_HOST from our rhoas secret. Adding this here overrides the value that we hardcoded in the earlier. * mp.messaging.connector.smallrye-kafka.security.protocol and mp.messaging.connector.smallrye-kafka.sasl.mechanism are used to secure the connection and enable authentication via SASL since RHOSAK is secured. The of the Kafka documentation explains these values in more detail. * mp.messaging.connector.smallrye-kafka.sasl.jaas.config sets up JAAS configuration to provide the RHOAS_SERVICE_ACCOUNT_CLIENT_ID and RHOAS_SERVICE_ACCOUNT_CLIENT_SECRET from our rhoas secret to autheniticate with RHOSAK. So in short the above configuration makes values from our secret available to WildFly, overrides the location of the Kafka server, and adds more MicroProfile Config properties to enable SSL and authentication. DEPLOYING OUR APPLICATION Now that we have configured everything properly, it is time to test our application! First you will need to install , and use it to add the wildfly Helm repostory as outlined in Then from the root folder of your local copy of the example repository, run: $ helm install rhosak-example -f ./helm.yml wildfly/wildfly This will return quickly but that does not mean the application is up and running yet. Check the application in the OpenShift console or using oc get deployment rhosak-example -w. Essentially what happens is it starts two pods. One for you application, and another which is doing the build of the bootable jar. Once the build one is done and has published the resulting image, the pod running the application can start properly. ACCESSING OUR APPLICATION RUNNING ON OPENSHIFT First we need the URL of our application on OpenShift: $ oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD rhosak-example rhosak-example-kkhan1-dev.apps.sandbox.x8i5.p1.openshiftapps.com rhosak-example &lt;all&gt; edge/Redirect None In my case the URL is rhosak-example-kkhan1-dev.apps.sandbox.x8i5.p1.openshiftapps.com. You should of course substitute that with the URL of your application in the following steps. Next, let’s add some entries using Curl: $ curl -X POST https://rhosak-example-kkhan1-dev.apps.sandbox.x8i5.p1.openshiftapps.com/one $ curl -X POST https://rhosak-example-kkhan1-dev.apps.sandbox.x8i5.p1.openshiftapps.com/two These will be sent to Kafka, and received again by the application which will keep a list of the most recently received values. Note that the https:// is needed - if left out, the commands will appear to work, but no data will actually be posted. To read this list of recently received values, we can run Curl again: $ curl https://rhosak-example-kkhan1-dev.apps.sandbox.x8i5.p1.openshiftapps.com [one, two] CONCLUSION Compared to running locally the RHOSAK steps look a lot more involved. However, we have achieved a lot! If we break down what we have actually done, it looks simpler: * Use rhoas to set up Kafka, a topic, and a service account authorised to publish/consume messages * Create a secret called rhoas containing the location of the Kafka instance and credentials to access it * Configure our application to use it by: * Mounting the secret under /etc/config/rhoas in the Helm Chart * Use org.wildfly.plugins:wildfly-jar-maven-plugin to * provision a trimmed down server with the required functionality * run a CLI script when building the server to mount the /etc/config/rhoas folder as a MicroProfile Config ConfigSource and use values from that to override the location of the server, and add properties to turn on SSL, SASL authentication, and provide the credentials from our secret to authenticate I hope this guide will be helpful to people wanting to try RHOSAK from WildFly for the first time.</content><dc:creator>Kabir Khan</dc:creator></entry><entry><title type="html">a DMN FEEL handbook</title><link rel="alternate" href="https://blog.kie.org/2022/08/a-dmn-feel-handbook.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2022/08/a-dmn-feel-handbook.html</id><updated>2022-08-18T07:10:49Z</updated><content type="html">a DMN FEEL handbook “promotion” video 🙂 We’re introducing an (experimental) DMN FEEL handbook, an helpful companion for your DMN modeling activities! You can access this new helpful resource at the following URL: . Key features include: * FEEL built-in functions organised by category * tested and integrated FEEL examples * Responsive design: easily access on Mobile, Tablet and Desktop from your favourite browser! …and many more! IMPLEMENTATION DETAILS For the technically curious, this section will highlight some of the technical implementation choices for the realisation of this handbook. If you want to just use this handbook, you are free to skip this section! 🙂 The framework used to render this handbook is called ; it’s very convenient to use it when, in general, you want a neat API documented with use-cases and examples. This makes it a perfect framework candidate to build a technical manual of a very specific aspect –the FEEL expression language of DMN. Naturally we’re keeping Antora for the general documentation of Drools, which is more powerful for the user-manuals and modularising content. Then, in order to CI the FEEL snippets and examples in the handbook, we’re leveraging the awesome ! As you can see in the Java_script source () we’re parsing the content of the Markdown file to search for codeblocks related to FEEL. Each codeblock related to FEEL is then evaluated, to make sure it does not error or misbehave. This allowed us to catch small typos, which we corrected in the main docs. I got the inspiration to implement this approach by reading the Rust manual. If you are familiar with that book, you will likely recognise where :). Finally, we integrated the JBang! tests with CI with a , and then we serve the content via GitHub Pages. CONCLUSIONS Don’t forget to checkout this DMN FEEL handbook today! Do you like it? Feedback? Let us know! The post appeared first on .</content><dc:creator>Matteo Mortari</dc:creator></entry><entry><title>A demonstration of Drogue IoT using Node.js</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/18/demonstration-drogue-iot-using-nodejs" /><author><name>Daniel Bevenius</name></author><id>a2f40cf1-6204-4671-b8d2-1c8c168bf5e1</id><updated>2022-08-18T07:00:00Z</updated><published>2022-08-18T07:00:00Z</published><summary type="html">&lt;p&gt;The goal of the &lt;a href="https://drogue.io/"&gt;Drogue IoT&lt;/a&gt; project is to make it easy to connect devices to cloud-based applications. This article will demonstrate how to implement firmware in Rust based on Drogue's device support. This way, a device can communicate with the cloud using the low power &lt;a href="https://lora-alliance.org/"&gt;LoRaWAN&lt;/a&gt; protocol. We will also illustrate how Node.js handles the server side.&lt;/p&gt; &lt;h2&gt;The purpose of Drogue IoT&lt;/h2&gt; &lt;p&gt;Many &lt;a href="https://developers.redhat.com/topics/open-source/"&gt;open source&lt;/a&gt; technologies already exist in the realm of messaging and the Internet of Things ( IoT). However, technologies change over time, and not everything that exists now is fit for the world of tomorrow. For instance, &lt;a href="https://developers.redhat.com/topics/c/"&gt;C and C++&lt;/a&gt; still have issues with memory safety. The concepts of cloud native, &lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;serverless&lt;/a&gt;, and pods might also need a different approach to designing cloud-side applications. Drogue IoT aims to help support these new environments.&lt;/p&gt; &lt;p&gt;&lt;a href="https://book.drogue.io/drogue-device/dev/index.html"&gt;Drogue Device&lt;/a&gt; is a firmware framework written in Rust with an actor-based programming model. &lt;a href="https://book.drogue.io/drogue-cloud/dev/index.html"&gt;Drogue Cloud&lt;/a&gt; is a thin layer of services that creates an IoT-friendly API for existing technologies such as &lt;a href="https://knative.dev/"&gt;Knative&lt;/a&gt; and &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; and a cloud-friendly API using &lt;a href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt; on the other side. The idea is to give you an overall solution ready to run IoT as a service. Figure 1 illustrates the Drogue IoT architecture.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch_5.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/arch_5.png?itok=D3a1IIwC" width="600" height="200" alt="An illustration of devices sending data that is transformed and exported." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Devices send data using standard protocols of Drogue Cloud, where they are transformed and exported. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;LoRaWAN network coverage&lt;/h2&gt; &lt;p&gt;LoRaWAN is a low-power wireless network that enables you to run a device on batteries for months, sending telemetry data to the cloud every now and then. To achieve this efficient connectivity, you need LoRaWAN network coverage, and &lt;a href="https://www.thethingsnetwork.org/"&gt;The Things Network&lt;/a&gt; (TTN) provides exactly that. You can extend the TTN network by running your gateway if your local area lacks coverage. TTN provides a public service that allows you to exchange data between devices and applications.&lt;/p&gt; &lt;h2&gt;Drogue Device&lt;/h2&gt; &lt;p&gt;Exchanging data with Drogue Device is easy. The following snippet focuses on the code  that exchanges data:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;let mut tx = String::&lt;heapless::consts::U32&gt;::new(); let led = match self.config.user_led.state().unwrap_or_default() { true =&gt; "on", false =&gt; "off", }; write!(&amp;mut tx, "ping:{},led:{}", self.counter, led).ok(); let tx = tx.into_bytes(); let mut rx = [0; 64]; let result = cfg .lora .request(LoraCommand::SendRecv(&amp;tx, &amp;mut rx)) .unwrap() .await;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice the &lt;code&gt;await&lt;/code&gt; keyword at the end? Yes, that is indeed asynchronous Rust. A hardware access layer (HAL) named &lt;a href="https://book.drogue.io/embassy/dev/index.html"&gt;Embassy&lt;/a&gt;, another Drogue IoT project, allows the program to run on the device, which in this example is an embedded &lt;a href="https://www.st.com/en/evaluation-tools/b-l072z-lrwan1.html"&gt;STM32 Cortex-M0 board&lt;/a&gt;. Thanks to Embassy and the drivers in Drogue Device, asynchronous programming becomes pretty simple. And thanks to Rust, your code is less likely to cause any undefined behavior, like corrupted memory.&lt;/p&gt; &lt;h2&gt;Node.js&lt;/h2&gt; &lt;p&gt;The cloud side of the IoT application needs a simple "reconcile loop." The device reports its current state, and you derive the desired state from that. The information received might result in a command you send back to the device.&lt;/p&gt; &lt;p&gt;The application in this article is pretty much the same as &lt;a href="https://developers.redhat.com/articles/2021/06/10/connect-quarkus-applications-drogue-iot-and-lorawan"&gt;connect-quarkus-applications-drogue-iot-and-lorawan&lt;/a&gt; written by Jens Reimann. But his version uses the Quarkus Java framework as the backend implementation, whereas our application uses Node.js.&lt;/p&gt; &lt;p&gt;The entry point of the application is &lt;code&gt;index.js&lt;/code&gt;, which configures and starts an HTTP server and an MQTT client. The HTTP server serves content from the static directory, which contains an &lt;code&gt;index.html&lt;/code&gt; file shown in the screenshot below. This file contains a &lt;code&gt;&lt;script&gt;&lt;/code&gt; element that uses Server Sent Events (SSE) to allow the server to send updates to it. In addition to serving the static content, the  HTTP server sends events through SSE. &lt;a href="https://www.fastify.io/"&gt;Fastify&lt;/a&gt; builds the server, and &lt;a href="https://www.npmjs.com/package/fastify-sse"&gt;fastify-sse&lt;/a&gt; handles the SSE.&lt;/p&gt; &lt;p&gt;The MQTT client handles a message event as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;client.on('message', (receiveTopic, message) =&gt; { const json = JSON.parse(message); const framePayload = Buffer.from(json.data.uplink_message.frm_payload, 'base64'); const event = { deviceId: json.device, timestamp: json.time, payload: framePayload.toString('utf8') }; sse.sendMessageEvent(event); if (event.payload.startsWith('ping')) { const command = { deviceId: event.deviceId, payload: getPayload(event, sse) }; sse.updateResponse(sse.lastResponse); sse.sendCommandEvent(command); const sendTopic = `command/${appName}/${command.deviceId}/port:1`; const responsePayload = Buffer.from(command.payload, 'utf8'); client.publish(sendTopic, responsePayload, {qos: QOS_AT_LEAST_ONCE}); } });&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Pretty simple, isn't it? For more details on the Node.js implementation, please see the &lt;a href="https://book.drogue.io/drogue-workshops/ttn-lorawan/nodejs-application.html"&gt;ttn-lorawan&lt;/a&gt; workshop.&lt;/p&gt; &lt;h2&gt;Drogue Cloud&lt;/h2&gt; &lt;p&gt;So far, the code shown in this article is fairly straightforward, focusing on our use case. However, we are missing a big chunk in the middle. How do we connect Node.js with the actual device? Sure, we could recreate all that ourselves, implementing the TTN API, registering devices, and processing events. Alternatively, we could simply use Drogue Cloud and let it do the plumbing for us.&lt;/p&gt; &lt;p&gt;Creating a new application and device is easy using the &lt;code&gt;drg&lt;/code&gt; command-line tool. Installation instructions are on the &lt;a href="https://github.com/drogue-iot/drg#installation"&gt;drg installation page&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ drg create application my-app $ drg create device --app my-app my-device&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The device registry in Drogue Cloud not only stores device information but can also reconcile with other services. Adding the following information makes it sync with TTN:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ drg create application my-app --spec '{ "ttn": { "api": { "apiKey": "...", "owner": "my-ttn-username", "region": "eu1" } } }' $ drg create --app my-app device my-device --spec '{ "ttn": { "app_eui": "0123456789ABCDEF", "dev_eui": "ABCDEF0123456789", "app_key": "0123456789ABCDEF...", "frequency_plan_id": "...", "lorawan_phy_version": "PHY_V1_0", "lorawan_version": "MAC_V1_0" } }'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This code creates a new TTN application, registers the device, sets up a webhook, creates the gateway configuration in Drogue Cloud, and ensures that credentials are present and synchronized.&lt;/p&gt; &lt;h2&gt;Learn more in the LoRaWAN end-to-end workshop&lt;/h2&gt; &lt;p&gt;Did that seem a bit fast? Yes, indeed! This is a lot of information for a single article, so we focused on the essential parts. We put together everything you need to know in the &lt;a href="https://book.drogue.io/drogue-workshops/ttn-lorawan/index.html"&gt;LoRaWAN end-to-end workshop&lt;/a&gt;, which provides more detail and background information. By the end of that workshop, you should have a web front-end to control your device, as shown in Figure 2. Most importantly, you will have a solid foundation for creating your own applications on top of Drogue IoT.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/web.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/web.png?itok=XIsruLc7" width="1440" height="523" alt="A screenshot of the application displaying messages received from the device." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The application displays messages received from the device. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;We hope you enjoyed this article. Now, you are ready to get started with Node.js and Drogue IoT. To learn more about what Red Hat is up to on the Node.js front, please explore our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/18/demonstration-drogue-iot-using-nodejs" title="A demonstration of Drogue IoT using Node.js "&gt;A demonstration of Drogue IoT using Node.js &lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniel Bevenius</dc:creator><dc:date>2022-08-18T07:00:00Z</dc:date></entry><entry><title>How to easily deploy JBoss EAP on Azure</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/17/how-ansible-simplifies-jboss-eap-deployment-azure" /><author><name>Harsha Cherukuri</name></author><id>508c1342-d157-4be8-9332-a27590fde406</id><updated>2022-08-17T07:00:00Z</updated><published>2022-08-17T07:00:00Z</published><summary type="html">&lt;p&gt;This article demonstrates how to deploy &lt;a href="https://developers.redhat.com/products/eap/download"&gt;Red Hat JBoss Enterprise Application Platform (JBoss EAP)&lt;/a&gt; on Microsoft Azure using &lt;a href="https://www.ansible.com"&gt;Ansible&lt;/a&gt; automation. Currently, Red Hat has a &lt;a href="https://azuremarketplace.microsoft.com/marketplace/apps/redhat.jboss-eap-rhel"&gt;Microsoft Azure Marketplace offering of JBoss EAP&lt;/a&gt;, but it's available only through the &lt;a href="https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/redhat/byos"&gt;Bring-Your-Own-Subscription (BYOS)&lt;/a&gt; model, which is relatively complex. This article creates Azure resources using &lt;a href="https://docs.ansible.com/ansible/latest/collections/azure/azcollection/index.html"&gt;Ansible Collections for Azure&lt;/a&gt; and then deploys JBoss EAP using the &lt;a href="https://github.com/ansible-middleware/wildfly"&gt;WildFly&lt;/a&gt; service provided by the &lt;a href="https://ansiblemiddleware.com/"&gt;Ansible Middleware project&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Deploy JBoss EAP in 6 easy steps&lt;/h2&gt; &lt;p&gt;We will use &lt;a href="https://github.com/ansible-middleware/azure-eap-demo"&gt;azure-eap-demo&lt;/a&gt; as the sample application for this article. Using this application, we will automate and deploy JBoss EAP instances on Azure virtual machines running &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Step 1.  Prerequisites setup&lt;/h3&gt; &lt;p&gt;To run the sample application, please put the following requirements in place:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An Azure account with an active subscription. If you do not have an Azure subscription, &lt;a href="https://azure.microsoft.com/pricing/free-trial"&gt;create one for free&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;JBoss EAP: You need a Red Hat account with a Red Hat Subscription Management entitlement for JBoss EAP. This entitlement allows you to download a version of JBoss EAP tested and certified by Red Hat. If you do not have a JBoss EAP entitlement, sign up for a free developer subscription:  &lt;a href="https://developers.redhat.com/register"&gt;Red Hat Developer Subscription for Individuals&lt;/a&gt;. Once registered, you can find the necessary credentials (pool IDs) at the &lt;a href="https://access.redhat.com/management/"&gt;Red Hat customer portal&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;The following software on the controller host or local machine: &lt;ul&gt; &lt;li&gt;Ansible (version 2.9 or greater)&lt;/li&gt; &lt;li&gt;Python (version 3.9 or greater)&lt;/li&gt; &lt;li&gt;Python3 &lt;a href="https://pypi.org/project/netaddr/"&gt;netaddr&lt;/a&gt; (obtained using dnf or pip)&lt;/li&gt; &lt;li&gt;The &lt;a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli"&gt;Azure command-line interface&lt;/a&gt; (CLI)&lt;/li&gt; &lt;li&gt;Download the &lt;a href="https://github.com/ansible-middleware/azure-eap-demo"&gt;azure-eap-demo application&lt;/a&gt; to your local machine.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Step 2.  Install WildFly and other components&lt;/h3&gt; &lt;p&gt;After you unpack the &lt;code&gt;azure-eap-demo&lt;/code&gt; application, change into the repository's top-level directory, and run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection install -r requirements.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We are using a dynamic inventory provided by Azure. Once the instances are created, you can view the inventory with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-inventory -i inventory/myazure_rm.yml --graph&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 3.  Create credentials for Red Hat portal access&lt;/h3&gt; &lt;p&gt;You need to provide credentials in the playbook so that it can download software from the Red Hat customer portal. Specify your Red Hat account name in the &lt;code&gt;rhn_username&lt;/code&gt; variable and your password in the &lt;code&gt;rhn_password&lt;/code&gt; variable.&lt;/p&gt; &lt;p&gt;In addition, you have to specify the Red Hat Subscription Management entitlement for JBoss EAP in the &lt;code&gt;jboss_eap_rhn_id&lt;/code&gt; variable. This variable allows you to specify which version of JBoss EAP (supported by Red Hat) you would like to install. Alternatively, you can just download and install the JBoss EAP ZIP file from the Red Hat customer portal.&lt;/p&gt; &lt;p&gt;All these variables can be stored in a YAML file, whose name you specify in a section in the Ansible playbook named &lt;code&gt;vars_files&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Step 4.  Run the Ansible playbook&lt;/h3&gt; &lt;p&gt;Now run the Ansible playbook in &lt;code&gt;create-demo-setup.yml &lt;/code&gt;which creates resources on Azure and deploys JBoss EAP:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;​​​​​​​$ ansible-playbook -e @rhn-creds.yml -i inventory/myazure_rm.yml -e "ansible_ssh_user=rheluser ansible_ssh_private_key_file='provide_your_ssh_private_key' hosts_group_name=eap wildfly_version=7.4 override_install_name=jboss-eap" create-demo-setup.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As part of the playbook execution, the &lt;a href="https://github.com/ansible-middleware/azure-eap-demo"&gt;azure-eap-demo&lt;/a&gt; repository is cloned. Its &lt;code&gt;create-demo-setup.yml&lt;/code&gt; file contains:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- name: Create Azure VM hosts: localhost gather_facts: false connection: local vars: repo_url: "https://github.com/ansible-middleware/wildfly-cluster-demo.git" branch: main tasks: - name: Git checkout ansible.builtin.git: repo: "{{ repo_url }}" dest: "{{ playbook_dir }}/wildfly-cluster-demo" version: "{{ branch }}" single_branch: yes clone: yes update: yes - name: Create demo resources on azure. include_role: name: 'azure' vars: ssh_key_path: "{{ ssh_key | default(lookup('env', 'HOME') + '/.ssh/id_rsa.pub')}}" - meta: refresh_inventory - pause: minutes: 1 - name: Run wildfly-cluster-demo import_playbook: wildfly-cluster-demo/playbook.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This playbook creates the Azure resources the application needs, including a resource group, virtual networks, subnets, security groups, network interfaces, three virtual machines running Red Hat Enterprise Linux, and public IP addresses for the virtual machines.&lt;/p&gt; &lt;p&gt;All the default parameters for the Azure cloud instances are within the installed package: &lt;code&gt;roles/azure/defaults/main.yml&lt;/code&gt; ​​​​​​.&lt;/p&gt; &lt;p&gt;Finally, the playbook deploys the WildFly cluster demo. Refer to the article &lt;a href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible"&gt;Automate and deploy a JBoss EAP cluster with Ansible&lt;/a&gt; to learn more about how to use WildFly.&lt;/p&gt; &lt;h3&gt;Step 5.  Verify deployment of the JBoss EAP cluster and application&lt;/h3&gt; &lt;p&gt;Once the playbook completes successfully, you can verify the JBoss EAP cluster by logging into the &lt;a href="https://portal.azure.com/"&gt;Azure portal&lt;/a&gt;. Here, you will find all the resources created to support the JBoss EAP cluster. Log in or SSH into any of the virtual machines created and confirm that the WildFly service is running and accessible. Alternatively, you can run the &lt;code&gt;validate.yml&lt;/code&gt; playbook provided in the &lt;a href="https://github.com/ansible-middleware/wildfly-cluster-demo"&gt;wildfly-cluster-demo&lt;/a&gt; application to validate the configuration.&lt;/p&gt; &lt;h3&gt;Step 6.  Clean up Azure resources&lt;/h3&gt; &lt;p&gt;To clean up all the resources created on Azure, run the &lt;code&gt;clean-demo-resources.yml&lt;/code&gt; playbook:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ansible-playbook clean-demo-resources.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The contents of &lt;code&gt;clean-demo-resources.yml&lt;/code&gt; are as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- name: Create Azure VM hosts: localhost connection: local tasks: - name: Create VM's on azure. include_role: name: 'azure' vars: action: destroy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;destroy&lt;/code&gt; action runs the &lt;code&gt;roles/azure/tasks/destroy.yml&lt;/code&gt; file, which contains:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: Remove a VM and all resources that were autocreated azure_rm_virtualmachine: resource_group: "{{ item.resourcegroup_name }}" name: "{{ item.name }}" remove_on_absent: all_autocreated state: absent loop: "{{ vm }}" - name: Delete a resource group including resources it contains azure_rm_resourcegroup: name: "{{ item.name }}" force_delete_nonempty: yes state: absent loop: "{{ resource_groups }}"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The playbook removes all the virtual machines and deletes all the resources under the &lt;code&gt;eap-cluster&lt;/code&gt; resource group.&lt;/p&gt; &lt;h2&gt;Ansible simplifies deployment on Azure&lt;/h2&gt; &lt;p&gt;In this article, we demonstrated a step-by-step process to create resources using Ansible on Microsoft Azure and deploy a JBoss EAP cluster using tooling from the Ansible Middleware project. Check out the other collections and demos within the &lt;a href="https://github.com/ansible-middleware"&gt;ansible-middleware&lt;/a&gt; GitHub organization and the &lt;a href="https://ansiblemiddleware.com"&gt;Ansible Middleware website&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/17/how-ansible-simplifies-jboss-eap-deployment-azure" title="How to easily deploy JBoss EAP on Azure"&gt;How to easily deploy JBoss EAP on Azure&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Harsha Cherukuri</dc:creator><dc:date>2022-08-17T07:00:00Z</dc:date></entry><entry><title>Quarkus Newsletter #23 - August</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-newsletter-23/&#xA;            " /><author><name>James Cobb (https://twitter.com/insectengine)</name></author><id>https://quarkus.io/blog/quarkus-newsletter-23/</id><updated>2022-08-17T00:00:00Z</updated><published>2022-08-17T00:00:00Z</published><summary type="html">The August Quarkus Newletter has been released! What’s new this month? Plenty! Get a great understanding of OpenTelemetry with Eric Deandrea’s article "OpenTelemetry: A Quarkus Superheroes demo of Observability". Get the good and bad experiences of a developer jumping into the deep end on a project with María Arias de...</summary><dc:creator>James Cobb (https://twitter.com/insectengine)</dc:creator><dc:date>2022-08-17T00:00:00Z</dc:date></entry><entry><title type="html">Getting started with AtlasMap</title><link rel="alternate" href="http://www.mastertheboss.com/java/getting-started-with-atlasmap/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/getting-started-with-atlasmap/</id><updated>2022-08-16T16:39:32Z</updated><content type="html">This article is a whirlwind tour of AtlasMap Data transformation API and User Interface. We will learn how to use its editor to define mapping rules and how to use them in a sample Java Integration project. Data transformation is the process of converting data from one format or structure into another format or structure. ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Connect MongoDB to a Node.js application with kube-service-bindings</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/16/connect-mongodb-nodejs-application-kube-service-bindings" /><author><name>Costas Papastathis, Michael Dawson</name></author><id>75fcba18-2deb-4b89-b6c2-c6e2fc0932a6</id><updated>2022-08-16T07:00:01Z</updated><published>2022-08-16T07:00:01Z</published><summary type="html">&lt;p&gt;This is the third and final article in a three-part series introducing &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt; for &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; developers on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. Together with the &lt;a href="https://operatorhub.io/operator/service-binding-operator"&gt;Service Binding Operator&lt;/a&gt; (SBO), kube-service-bindings makes it easier to share credentials for services with your applications.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2022/06/29/connect-services-kubernetes-easily-kube-service-bindings"&gt;first article of this series&lt;/a&gt; offered background on the tools we're using, and the &lt;a href="https://developers.redhat.com/articles/2022/07/22/enable-backing-services-kubernetes-service-service-binding-operator-and-kube"&gt;second&lt;/a&gt; set up some basic elements such as hosting on the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; and a &lt;a href="https://www.mongodb.com"&gt;MongoDB&lt;/a&gt; database. Now we're going to use all these tools to create a binding between our Node.js application and the database.&lt;/p&gt; &lt;h2&gt;Deploy MongoDB as a cloud-hosted database&lt;/h2&gt; &lt;p&gt;The previous article set up access between &lt;a href="https://www.mongodb.com/cloud/atlas/lp/try7?utm_source=google&amp;utm_campaign=gs_americas_united_states_search_core_brand_atlas_desktop&amp;utm_term=mongodb%20atlas&amp;utm_medium=cpc_paid_search&amp;utm_ad=e&amp;utm_ad_campaign_id=12212624338&amp;adgroup=115749704063&amp;gclid=Cj0KCQjw8uOWBhDXARIsAOxKJ2Hyk1VN_iVHDUUBWqUwwxLeVOw2WtbigZMayXJdUmnptIUnJUod8xAaAs1MEALw_wcB"&gt;MongoDB Atlas&lt;/a&gt; and your OpenShift cluster. If you went through those steps successfully, you are ready to deploy a cloud-hosted MongoDB database in the cluster as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In the OpenShift console, visit the Topology view by selecting &lt;strong&gt;Developer→Topology&lt;/strong&gt; from the left sidebar.&lt;/li&gt; &lt;li&gt;Select the project where you would like to deploy the cloud database by selecting, from the top of the left sidebar, &lt;strong&gt;Developer&lt;/strong&gt;→&lt;strong&gt;Topology&lt;/strong&gt;→&lt;strong&gt;Project&lt;/strong&gt;. From the dropdown menu, select your project.&lt;/li&gt; &lt;li&gt;In the left sidebar menu, click &lt;strong&gt;+Add→Cloud-Hosted Database→MongoDB Atlas Cloud Database Service→Add to Topology&lt;/strong&gt;. Select your database instance and click &lt;strong&gt;Add to topology→Continue&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Upon successful connection, you are taken to the Topology view, where the cloud-hosted database is deployed and visible (Figure 1).&lt;/li&gt; &lt;/ol&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/mongo_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/mongo_0.png?itok=oJytAkSx" width="341" height="284" alt="The Topology view shows that MongoDB Atlas is now accessible in your cluster." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The Topology view shows that MongoDB Atlas is now accessible in your cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Deploy the Node.js application in OpenShift&lt;/h2&gt; &lt;p&gt;There are several ways to deploy a Node.js application in OpenShift: Through the &lt;code&gt;oc&lt;/code&gt; OpenShift command-line interface (CLI), the &lt;code&gt;odo&lt;/code&gt; CLI, the OpenShift console, etc. This article covers two options: The OpenShift console and &lt;a href="https://www.npmjs.com/package/nodeshift"&gt;Nodeshift&lt;/a&gt;, an NPM package.&lt;/p&gt; &lt;h3&gt;Deploy through the OpenShift console&lt;/h3&gt; &lt;p&gt;From the &lt;strong&gt;Developer&lt;/strong&gt; perspective, select &lt;strong&gt;+Add→Import from Git&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In the &lt;strong&gt;Git Repo url&lt;/strong&gt; field, set the repository URL to &lt;code&gt;https://github.com/nodeshift-blog-examples/kube-service-bindings-examples&lt;/code&gt;. This is a &lt;a href="https://github.com/nodeshift-blog-examples/kube-service-bindings-examples"&gt;kube-service-bindings examples repository&lt;/a&gt; maintained by our team; it contains the Node.js application you are deploying in this article.&lt;/p&gt; &lt;p&gt;Expand &lt;strong&gt;Show advanced Git options&lt;/strong&gt;. On the &lt;strong&gt;Context dir&lt;/strong&gt; field, set the value to &lt;code&gt;src/mongodb&lt;/code&gt;, which is the path of the subdirectory where your Node.js application is located.&lt;/p&gt; &lt;p&gt;On &lt;strong&gt;Builder Image&lt;/strong&gt;, select &lt;strong&gt;Node.js&lt;/strong&gt; and click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt; &lt;h3&gt;Deploy through Nodeshift&lt;/h3&gt; &lt;p&gt;Open a terminal and clone the git repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/nodeshift-blog-examples/kube-service-bindings-examples.git $ cd ./kube-service-bindings-examples/src/mongodb&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Install Nodeshift globally:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ npm install -g nodeshift&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To find the login credentials required by the next command you'll use, visit your OpenShift console. In the upper right corner, click your username. A dropdown will appear. Click &lt;strong&gt;Copy login command&lt;/strong&gt; (Figure 2), which transfers you to another page. Then click &lt;strong&gt;Display Token&lt;/strong&gt; to display the username, password, and server credentials to log in with Nodeshift.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/copy_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/copy_1.png?itok=6SxN8H8y" width="378" height="209" alt="Under your name in the console, you can obtain login credentials." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Under your name in the console, you can obtain login credentials. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Using these credentials, you can now log in to your OpenShift cluster with Nodeshift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ nodeshift login --username=developer --password=password --server=https://api.server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Deploy the Node.js application with Nodeshift through the following command, replacing the namespace name with your specific project name:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ nodeshift --namespace.name=&lt;selected-project&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your application should be deployed and visible in the Topology view, as shown in Figure 3.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/node.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/node.png?itok=CJxGzdXs" width="227" height="236" alt="The Node.js application appears in the Topology view." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The Node.js application appears in the Topology view. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Establish a connection between the Node.js application and the MongoDB database&lt;/h2&gt; &lt;p&gt;The final step in this series is to establish a connection between your Node.js application and the MongoDB database, which we'll accomplish in this section.&lt;/p&gt; &lt;h3&gt;Service Binding Operator&lt;/h3&gt; &lt;p&gt;At this point, two instances should show up in your Topology view: the Node.js application and the connection to your MongoDB instance in Atlas (Figure 4).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/two_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/two_0.png?itok=uWDdr5gL" width="675" height="284" alt="The Topology view shows both the Node.js application and the external MongoDB database." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The Topology view shows both the Node.js application and the external MongoDB database. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;To establish a connection between these instances, you will use the Service Binding Operator to share the credentials and kube-service-bindings to parse those credentials (binding data).&lt;/p&gt; &lt;p&gt;You can create a Service Binding in two different ways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drag a line in the Topology view between the two backing services (the Node.js application and MongoDB).&lt;/li&gt; &lt;li&gt;Apply a YAML file specifying the service binding.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We will go with the first option, which in our case is faster and easier.&lt;/p&gt; &lt;p&gt;Hover the mouse over the Node.js application in the Topology view. An arrow should appear. Drag the arrow from the Node.js application to the circle around the MongoDB instance. A tooltip should be visible that says &lt;strong&gt;Create service binding&lt;/strong&gt;. Release the mouse button and a pop-up box will let you specify the name of the service binding. Click &lt;strong&gt;Create binding&lt;/strong&gt;. The container of the Node.js application will restart immediately (Figure 5).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Check the environment of Node.js application by clicking the Node.js application container in the Topology view. In the right sidebar, click &lt;strong&gt;Resources→View Logs (Pods Section)&lt;/strong&gt; and visit the &lt;strong&gt;Environment&lt;/strong&gt; tab. The &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; environment variable should be set, as shown in Figure 6.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;kube-service-bindings&lt;/h3&gt; &lt;p&gt;The final step is to read the binding data under the directory indicated by the &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; variable and pass the data to the MongoDB client to establish a connection to the MongoDB database. Your Node.js application already has kube-service-bindings as a dependency. So calling the &lt;code&gt;getBinding()&lt;/code&gt; function, as shown in the following JavaScript code snippet, does all the hard work of parsing, cleaning, and transforming the binding data into a consumable format for the MongoDB client:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;const { MongoClient } = require("mongodb"); const serviceBindings = require("kube-service-bindings"); const { url, connectionOptions } = serviceBindings.getBinding("MONGODB", "mongodb"); const mongoClient = new MongoClient(url, connectionOptions);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That's it. By visiting the URL of the Node.js application (click the arrow-box icon on the node), you can perform simple CRUD operations through the UI on the database.&lt;/p&gt; &lt;h2&gt;Easier integration with services on Kubernetes&lt;/h2&gt; &lt;p&gt;Over the past year, our team has been active in developing kube-service-bindings, making it easier for developers with little or no experience in managing containerized applications to securely share credentials among backing services.&lt;/p&gt; &lt;p&gt;Complementing the work on &lt;a href="https://github.com/nodeshift/kube-service-bindings"&gt;kube-service-bindings development&lt;/a&gt;, our team provides &lt;a href="https://github.com/nodeshift-blog-examples/kube-service-bindings-examples"&gt;examples&lt;/a&gt; for most of the clients supported by kube-service-bindings, instructions on utilizing kube-service-bindings, and a description of how to deploy a variety of backing services through Nodeshift in Kubernetes and OpenShift environments.&lt;/p&gt; &lt;p&gt;This series of articles has shown which clients are supported and how both a service binding and kube-service-bindings work. We guided you through the whole cycle of deploying a Node.js application backing service using the SBO and kube-service-bindings, sharing and parsing credentials for a connection between a Node.js application and a MongoDB database. kube-service-bindings read, parsed, and transformed binding data projected by the Service Binding Operator, returning data in a form directly consumable by the MongoDB client.&lt;/p&gt; &lt;p&gt;To help you use kube-service-bindings in other types of deployments, we have provided additional &lt;a href="https://github.com/nodeshift-blog-examples/kube-service-bindings-examples"&gt;Node.js examples&lt;/a&gt;. We hope you found this article interesting and now have a better understanding of kube-service-bindings and service bindings in general.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, check out &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;our Node.js page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/16/connect-mongodb-nodejs-application-kube-service-bindings" title="Connect MongoDB to a Node.js application with kube-service-bindings"&gt;Connect MongoDB to a Node.js application with kube-service-bindings&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Costas Papastathis, Michael Dawson</dc:creator><dc:date>2022-08-16T07:00:01Z</dc:date></entry><entry><title>How to set up Packit to simplify upstream project integration</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/16/how-set-packit-simplify-upstream-project-integration" /><author><name>Laura Barcziova</name></author><id>e727f048-6620-4039-9cca-83550b9309cc</id><updated>2022-08-16T07:00:00Z</updated><published>2022-08-16T07:00:00Z</published><summary type="html">&lt;p&gt;If you use open source projects from GitHub or GitLab in your infrastructure, you have probably established workflows to create project builds that you can install. You might create builds regularly and execute them in a similar manner across projects. Let's say you want to build the code changes in &lt;a href="https://getfedora.org"&gt;Fedora Linux&lt;/a&gt; or &lt;a href="https://www.centos.org/centos-stream/"&gt;CentOS Stream&lt;/a&gt; for each commit, release, or pull request in these projects. &lt;a href="https://packit.dev/"&gt;Packit&lt;/a&gt; can automate this policy, fold into a &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt; pipeline, and do even more.&lt;/p&gt; &lt;p&gt;Packit is an open source project that tests and builds RPM packages on Fedora Linux, CentOS Stream, and other distributions to ease the integration of upstream projects with the distributions.&lt;/p&gt; &lt;p&gt;This article focuses on &lt;a href="https://packit.dev/docs/packit-service/"&gt;Packit Service&lt;/a&gt;, which operates on GitHub and GitLab. You can also install a &lt;a href="https://packit.dev/docs/cli/"&gt;command line interface (CLI)&lt;/a&gt; locally to run Packit on your desktop or laptop.&lt;/p&gt; &lt;h2&gt;Setting up Packit in 3 steps&lt;/h2&gt; &lt;p&gt;Setting up Packit is pretty straightforward. Follow these three steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a valid &lt;a href="https://fedoraproject.org/wiki/Account_System"&gt;Fedora Account System &lt;/a&gt;account (if you don't already have one).&lt;/li&gt; &lt;li&gt;Install our GitHub application on &lt;a href="https://github.com/marketplace/packit-as-a-service"&gt;GitHub Marketplace&lt;/a&gt;, or &lt;a href="https://packit.dev/docs/guide/#how-to-set-up-packit-on-gitlab"&gt;configure a webhook&lt;/a&gt; on GitLab (depending on where your project lives).&lt;/li&gt; &lt;li&gt;Provide your FAS username, which will be verified (on Github, the verification is automatic).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That's all! Now you can add a configuration file to your project's repository and start setting up &lt;a href="https://packit.dev/docs/configuration/#packit-service-jobs"&gt;Packit service jobs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Most jobs require an &lt;a href="https://rpm-packaging-guide.github.io/#what-is-a-spec-file"&gt;RPM spec file&lt;/a&gt;. You can either place the spec file directly in your upstream repository or tell Packit how to download it from somewhere else.&lt;/p&gt; &lt;h2&gt;Packit for continuous integration&lt;/h2&gt; &lt;p&gt;Let's start with a simple example. You need to determine if the new code changes in GitHub or GitLab project built on all stable Fedora distros and CentOS Stream 9 will be successful. The process for such verifications is to get RPM builds for each pull request in your upstream project. The configuration file should include a build job like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;- job: copr_build trigger: pull_request targets: - fedora-stable-x86_64 - centos-stream-9-x86_64 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Easy, right? So what does this configuration accomplish? Packit takes the code changes for each pull request action, submits a new build in the &lt;a href="https://copr.fedorainfracloud.org/"&gt;Copr build system&lt;/a&gt;, and reports the results in the pull requests, as shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image1_10.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image1_10.png?itok=v9mQdDx1" width="925" height="347" alt="Packit creates commit checks that show results of the builds with links to details." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Packit creates commit checks that show results of the builds with links to details. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;You also have access to the actual Copr repository containing the builds. This access may be beneficial if you set up Packit to react to commits or releases because you can provide your RPM builds via the Copr repository to enable anyone to install them. You can also use Packit to do the builds in your Copr repository by specifying the owner and project name. Figure 2 shows one such repository.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image5_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image5_1.png?itok=BqZXrwOc" width="1161" height="564" alt="A Copr repository shows the latest builds for Cockpit projects." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. A Copr repository shows the latest builds for Cockpit projects. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Figure 3 shows a build by Packit in the repository.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image4_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image4_2.png?itok=VxBj-cem" width="809" height="498" alt="A Packit build displays general information about a Cockpit project build in the Copr repository." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. A Packit build displays general information about a Cockpit project build in the Copr repository. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Also, if you package your project in Fedora, you will be able to run scratch builds in a similar manner directly in &lt;a href="https://koji.fedoraproject.org"&gt;Koji&lt;/a&gt;, the Fedora build system.&lt;/p&gt; &lt;p&gt;Apart from the RPM builds, you can set up Packit to run your tests in the &lt;a href="https://docs.testing-farm.io/general/0.1/index.html"&gt;Testing Farm&lt;/a&gt; infrastructure. The tests can either use the built RPMs from Copr or run independently. You can read more information about tests in &lt;a href="https://packit.dev/docs/testing-farm/"&gt;the Packit documentation&lt;/a&gt;. Figure 4 shows how Packit reports test results.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image3_4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image3_4.png?itok=K4ciQ_LS" width="914" height="347" alt="Packit creates commit checks that show results of the tests with links to details." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Packit creates commit checks that show results of the tests with links to details. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Packit automation for Fedora downstream releases&lt;/h2&gt; &lt;p&gt;If you have ever maintained a package in Fedora, you probably know that releasing a new version downstream can be tedious. Packit can help you with boring tasks and do the repetitive work for you. With Packit, you can easily get your upstream releases into &lt;a href="https://src.fedoraproject.org"&gt;Fedora Package Sources&lt;/a&gt;, automatically submit builds in the &lt;a href="https://koji.fedoraproject.org/koji/"&gt;Koji build system&lt;/a&gt;, and create &lt;a href="https://bodhi.fedoraproject.org"&gt;Bodhi updates&lt;/a&gt;. If you are interested in this kind of automation, make sure to check our &lt;a href="https://packit.dev/docs/fedora-releases-guide/"&gt;release guide&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Ready to give Packit a try?&lt;/h2&gt; &lt;p&gt;If you are dealing with any of the situations mentioned, please check &lt;a href="https://packit.dev/docs/"&gt;our documentation&lt;/a&gt;, which will guide you through the Packit setup. If you have any questions, feel free to &lt;a href="https://packit.dev/#contact"&gt;contact us&lt;/a&gt;. We are happy to help and receive feedback or suggestions for features you would like to add.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/16/how-set-packit-simplify-upstream-project-integration" title="How to set up Packit to simplify upstream project integration"&gt;How to set up Packit to simplify upstream project integration&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Laura Barcziova</dc:creator><dc:date>2022-08-16T07:00:00Z</dc:date></entry><entry><title>How OpenShift Serverless Logic evolved to improve workflows</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/15/how-openshift-serverless-logic-evolved-improve-workflows" /><author><name>Daniel Oh, Simon Seagrave</name></author><id>8a532101-1b3e-4c4f-a8f4-e08ff8c689fe</id><updated>2022-08-15T07:00:00Z</updated><published>2022-08-15T07:00:00Z</published><summary type="html">&lt;p&gt;Serverless is an advanced cloud deployment model that aims to run business services on demand, enabling enterprises to save infrastructure costs tremendously. The benefit of serverless is an application designed and developed as abstract functions regardless of programming languages. This article describes how the serverless and function models have evolved since they were unleashed upon the world with AWS Lambda and what to look forward to with Red Hat OpenShift serverless logic.&lt;/p&gt; &lt;h2&gt;The 3 phases of serverless evolution&lt;/h2&gt; &lt;p&gt;As serverless technologies evolve, we at Red Hat created the evolutionary scale to help our customers better understand how serverless has grown and matured over time. The three phases of serverless evolution are as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Serverless 1.0 &lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p class="Indent1"&gt;At the beginning of the serverless era, the 1.0 phase, serverless was thought of as functions with tiny snippets of code running on demand for a short period. AWS Lambda made this paradigm popular, but it had limitations in terms of execution time, protocols, and runtimes.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Serverless 1.5&lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p class="Indent1"&gt;With the increase in popularity of &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;&lt;u&gt;Kubernetes&lt;/u&gt;&lt;/a&gt; running microservices on container platforms, the serverless era also moved forward to the 1.5 phase. This phase augmented serverless traits and benefits by deploying polyglot runtimes and container-based functions. The serverless 1.5 phase also delivered an abstraction layer to manage serverless applications using the open Kubernetes serverless community project, &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;&lt;u&gt;Knative&lt;/u&gt;&lt;/a&gt;. Red Hat was one of the founding members of the Knative project and continues to be one of the top contributors to that community. But this was not the end of the journey.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Serverless 2.0&lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p class="Indent1"&gt;We are now approaching the new serverless 2.0 phase. This phase involves more complex orchestration and integration patterns combined with some level of state management. Serverless functions are often thought of as stateless applications. But serverless workflows are designed for complex orchestrations of multiple services and functions, typically while preserving the state. The adoption of serverless increases as organizations perform more complex orchestrations. Consequently, the OpenShift Serverless team implements serverless workflows, utilizing our command of the business process automation space.&lt;/p&gt; &lt;h2&gt;The future of serverless workflows&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://serverlessworkflow.io/"&gt;&lt;u&gt;Serverless Workflow&lt;/u&gt;&lt;/a&gt; project is an open source specification that enables developers to design workflows running serverless functions using a standard domain-specific language (DSL). For increased flexibility, Serverless Workflows also allow developers to define business logic triggered by multiple events and services such as &lt;a href="https://cloudevents.io/"&gt;&lt;u&gt;CloudEvents&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://swagger.io/specification/"&gt;&lt;u&gt;OpenAPI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.asyncapi.com/"&gt;&lt;u&gt;AsyncAPI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://graphql.org/"&gt;&lt;u&gt;GraphQL&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://grpc.io/"&gt;&lt;u&gt;gRPC&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Red Hat is one of the &lt;a href="https://serverlessworkflow.io/community.html"&gt;&lt;u&gt;project maintainers&lt;/u&gt;&lt;/a&gt; of the Serverless Workflow project. We have been actively involved in innovation and contribution since the earliest days of the &lt;a href="https://www.cncf.io/"&gt;Cloud Native Computing Foundation (CNCF)&lt;/a&gt; project.&lt;/p&gt; &lt;p&gt;We are about to release a new feature of OpenShift Serverless called the serverless logic in developer preview. This feature allows developers to design workflows with serverless deployment and function development capabilities based on Knative and &lt;a href="https://kogito.kie.org/"&gt;&lt;u&gt;Kogito.&lt;/u&gt;&lt;/a&gt; Kogito augments function orchestration and automation to implement serverless workflows at scale on Red Hat OpenShift.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Stay tuned to the official &lt;a href="https://twitter.com/rhdevelopers"&gt;&lt;u&gt;@rhdevelopers Twitter stream&lt;/u&gt;&lt;/a&gt; and this blog for more details about our exciting new capability as we approach the release of Red Hat OpenShift Serverless Logic. We will also provide tutorials.&lt;/p&gt; &lt;p&gt;In the meantime, learn more about &lt;a href="https://developers.redhat.com/topics/serverless-java"&gt;OpenShift Serverless&lt;/a&gt;, and try it out by setting up your free and easy-to-use &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Red Hat Sandbox&lt;/a&gt; environment.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/15/how-openshift-serverless-logic-evolved-improve-workflows" title="How OpenShift Serverless Logic evolved to improve workflows"&gt;How OpenShift Serverless Logic evolved to improve workflows&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniel Oh, Simon Seagrave</dc:creator><dc:date>2022-08-15T07:00:00Z</dc:date></entry><entry><title type="html">First Steps with Dapr</title><link rel="alternate" href="http://www.ofbizian.com/2022/08/first-steps-with-dapr.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2022/08/first-steps-with-dapr.html</id><updated>2022-08-13T10:33:00Z</updated><content type="html">I recently to and work on the Dapr project. I about Dapr when it was initially announced by Microsoft, but hadn’t looked into it since it CNCF. Two years later, during my onboarding into the new role, I spent some time looking into it and here are the steps I took in the journey and my impressions so far. WHAT IS DAPR? TL;DR: Dapr is a distributed systems toolkit in a box. It addresses the peripheral integration concerns of applications and lets developers focus on the business logic. If you are familiar with Apache Camel, Spring Framework in the Java world, or other distributed systems frameworks, you will find a lot of similarities with Dapr. Here are a few parallels with other frameworks: * Similar to Camel, Dapr has connectors (called ) that let you connect to various external systems. * Similar to HashiCorp Consul, Dapr offers which can be backed by Consul. * Similar to Spring Integration, Spring Cloud, (remember Netflix Hystrix?) and many other frameworks, Dapr has error handling capabilities with retries, timeouts, circuit breakers which are called . * Similar to Spring Data KeyValue, Dapr offers Key/Value-based state abstractions. * Similar to Kafka, Dapr offers pub/sub-based service interactions. * Similar to ActiveMQ clients, Dapr offers , but these are not specific to a messaging technology, which means they can be used even with things such as AWS SQS or Redis for example. * Similar to Spring Cloud Config, Dapr offers configuration and secret management * Similar to Zookeeper or Redis clients, Dapr offers * Similar to a Service Mesh, Dapr offers mTLS and between your application and the sidecar. * Similar to Envoy, Dapr offers enhanced through automatic metrics, tracing and log collection. The primary difference between all of these frameworks and Dapr is that the latter offers its capabilities not as a library within your application, but as a sidecar running next to your application. These capabilities are exposed behind well-defined HTTP and gRPC APIs (very creatively called ) where the implementations (called ) can be swapped w/o affecting your application code. High-level Dapr architecture You could say, Dapr is a collection of stable APIs exposed through a sidecar and swappable implementations running somewhere else. It is the cloudnative incarnation of integration technologies that makes integration capabilities previously available only in a few languages, available to everybody, and portable everywhere: Kubernetes, on-premise, or literally (I mean the edge). GETTING STARTED The project is surprisingly easy to get up and running regardless of your developer background and language of choice. I was able to follow the getting started guides and run various quickstarts in no time on my MacOS. Here are roughly the steps I followed. INSTALL DAPR CLI Dapr CLI is the main tool for performing Dapr-related tasks such as running an application with Dapr, seeing the logs, running Dapr dashboard, or deploying all to Kubernetes. brew install dapr/tap/dapr-cli With the CLI installed, we have a few different options for installing and running Dapr. I’ll start from the least demanding and flexible option and progress from there. OPTION 1: INSTALL DAPR WITHOUT DOCKER This is the lightest but not the most useful way to run Dapr. dapr init --slim In this only daprd and placement binaries are installed on the machine which is sufficient for running Dapr sidecars locally. Run a Dapr sidecar The following command will start a Dapr sidecar called no-app listening on HTTP port 3500 and a random gRPC port. dapr run --app-id no-app --dapr-http-port 3500 Congratulations, you have your first Dapr sidecar running. You can see the sidecar instance through this command: dapr list or query its health status: curl -i http://localhost:3500/v1.0/healthz Dapr sidecars are supposed to run next to an application and not on their own. Let’s stop this instance and run it with an application. dapr stop --app-id no-app Run a simple app with a Dapr sidecar For this demonstration we will use a simple NodeJS : git clone cd samples/hello-dapr-slim npm install This is a Hello World the Dapr way and here is the gist of it: app.post('/neworder', bodyParser.json(), (req, res) =&gt; { const data = req.body.data; const orderId = data.orderId; res.status(200).send("Got a new order! Order ID: " + orderId); }); The application has one /neworder endpoint listening on port 3000. We can run this application and the sidecar with the following command: dapr run --app-id nodeapp --app-port 3000 --dapr-http-port 3500 node app.js The command starts the NodeJS application on port 3000 and Dapr HTTP endpoint on 3500. Once you see in the logs that the app has started successfully, we can poke it. But rather than hitting the /neworder endpoint directly on port 3000, we will instead interact with the application through the sidecar. We do that using Dapr CLI like this: dapr invoke --verb POST --app-id nodeapp --method neworder --data '{"data": { "orderId": "41" } }' And see the response from the app. If you noticed, the CLI only needs the app-id (instead of host and port) to locate where the service is running. The CLI is just a handy way to interact with your service. It that seems like too much magic, we can use bare-bones curl command too: curl -XPOST -d @sample.json -H "Content-Type:application/json" http://localhost:3500/v1.0/invoke/nodeapp/method/neworder This command uses the service Dapr’s invocation API to synchronously interact with the application. Here is a visual representation of what just happened: Invoking an endpoint through Dapr sidecar Now, with Dapr on the request path, we get the Daprized service invocation benefits such as resiliency policies such as retries, timeouts, circuit breakers, concurrency control; observability enhancements such as: metrics, tracing, logs; security enhancements such as mTLS, , etc. At this point, you can try out metadata, metrics endpoints, play with the options, or see your single microservice in Dapr dashboard. dapr dashboard The slim mode we are running on is good for the Hello World scenario, but not the best setup for local development purposes as it lacks state store, pub/sub, metric server, etc. Let’s stop the nodeapp using the command from earlier (or CTL +C), and remove the slim Dapr binary: dapr uninstall One thing to keep in mind is that this command will not remove the default configuration and component specification files usually located in: ~/.dapr folder. We didn’t create any files in the steps so far, but if you follow other tutorials and change those files, they will remain and get applied with every dapr run command in the future (unless overridden). This caused me some confusion, keep it in mind. OPTION 2: INSTALL DAPR WITH DOCKER This is the preferred way for running for development purposes but it requires Docker. Let’s set it up: dapr init The command will download and run 3 containers * Dapr placement container used with actors(I wish this was an optional feature) * Zipkin for collecting tracing information from our sidecars * And a single node Redis container used for state store, pub/sub, distributed-lock implementations. You can verify when these containers are running and you are ready to go. docker ps RUN THE QUICKSTARTS My next step from here was to try out the that demonstrate the building blocks for service invocation, pub/sub, state store, bindings, etc. The awesome thing about these quickstarts is that they demonstrate the same example in multiple ways: * With Dapr SDK and w/o any dependency to Dapr SDK i.e. using HTTP only. * In multiple languages: Java, Javascript, .Net, Go, Python, etc. You can mix and match different languages and interaction methods (SDK or native) for the same example which demonstrates Dapr’s polyglot nature. Option 3: Install Dapr on Kubernetes If you have come this far, you should have a good high-level understanding of what Dapr can do for you. The next step would be to deploy where most of the Dapr functionalities are available and closest to a production deployment. For this purpose, I used minikube locally with default settings and no custom tuning. dapr init --kubernetes --wait If successful, this command will start the following pods in dapr-system namespace: * dapr-operator: manages all components for state store, pub/sub, configuration, etc * dapr-sidecar-injector: injects dapr sidecars into deployment pods * dapr-placement: required with actors only. * dapr-sentry: manages mTLS between services and acts as a certificate authority. * dapr-dashboard: a simple webapp to explore what is running within a Dapr cluster These Pods collectively represent the Dapr . Injecting a sidecar From here on, adding a Dapr sidecar to an application (this would be Dapr dataplane) is as easy as adding the following to your Kubernetes Deployments:  annotations:    dapr.io/enabled: "true"    dapr.io/app-id: "nodeapp"    dapr.io/app-port: "3000" The dapr-sidecar-injector service watches for new Pods with the dapr.io/enabled annotation and injects a container with the daprd process within the pod. It also adds DAPR_HTTP_PORT and DAPR_GRPC_PORT environment variables to your container so that it can easily communicate with Dapr without hard-coding Dapr port values. To deploy a complete application on Kubernetes I suggest this step-by-step . It has a provider and consumer services and it worked the first time for me. TRANSPARENT VS EXPLICIT PROXY Notice Dapr sidecar injection is less intrusive than a typical service mesh with a transparent sidecar such as Istio’s Envoy. To inject a transparent proxy, typically the Pods also get injected with an init-container that runs at the start of the Pod and re-configures the Pods networking rules so that all ingress and egress traffic or your application container goes through the sidecar. With Dapr, that is not the case. There is a sidecar injected, but your application is in control of when and how to interact with Dapr over its well-defined explicit (non-transparent) APIs. Transparent service mesh proxies operate at lower network layers typically used by operations teams, whereas Dapr provides application layer primitives needed by developers. If you are interested in this topic, is a good explanation of the differences and overlaps of Dapr with services meshes. SUMMARY And finally, here are some closing thoughts with what I so far liked more and what less from Dapr. LIKED MORE * I love the fact that Dapr is one of the few CNCF projects targeting developers creating applications, and not only operations team who are running these applications. We need more cloudnative tools for implementing applications. * I love the non-intrusive nature of Dapr where capabilities are exposed over clear APIs and not through some black magic. I prefer transparent actions for instrumentation, observability, and general application insight, but not for altering application behavior. * I loved the polyglot nature of Dapr offering its capabilities to all programming languages and runtimes. This is what attracted me to Kubernetes and cloudnative in the first place. * I loved how easy it is to get started with Dapr and the many permutations of each quickstart. There is something for everyone regardless of where you are coming from into Dapr. * I’m excited about WASM and remote components features coming into Dapr. These will open new surface areas for more contributions and integrations. LIKED LESS * I haven’t used actors before and it feels odd to have a specific programming model included in a generic distributed systems toolkit. Luckily you don’t have to use it if you don’t want to. * The documentation is organized, but too sparse into multiple short pages. Learning a topic will require navigating a lot of pages multiple times, and it is still hard to find what you are looking for. Follow me at to join my journey of learning and using Dapr and shout out with any thoughts and comments.</content><dc:creator>Unknown</dc:creator></entry></feed>
